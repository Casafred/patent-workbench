# å¤§æ¨¡å‹APIç»Ÿä¸€ç®¡ç†æ•´åˆè®¡åˆ’

> æ–‡æ¡£ç‰ˆæœ¬: v1.3  
> åˆ›å»ºæ—¥æœŸ: 2026-02-28  
> æœ€åæ›´æ–°: 2026-02-28  
> çŠ¶æ€: ğŸ“‹ è§„åˆ’ä¸­

---

## ä¸€ã€é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 å½“å‰é—®é¢˜
- åŠŸèƒ½å…«(PDF-OCR)å‰ç«¯ç›´è°ƒAPIï¼Œç»•è¿‡åç«¯ä»£ç†ï¼Œå­˜åœ¨å®‰å…¨é£é™©
- API Keyè·å–è·¯å¾„ä¸ä¸€è‡´ï¼Œç»´æŠ¤å›°éš¾
- æ¨¡å‹é€‰æ‹©é€»è¾‘åˆ†æ•£ï¼Œæ— æ³•ç»Ÿä¸€é…ç½®
- ç¼ºå°‘å¤šæœåŠ¡å•†æ”¯æŒæ¶æ„ï¼Œéš¾ä»¥æ‰©å±•é˜¿é‡Œäº‘ç­‰ä¾›åº”å•†

### 1.2 æ•´åˆç›®æ ‡
1. **ç»Ÿä¸€APIè°ƒç”¨å…¥å£**: æ‰€æœ‰LLMè°ƒç”¨é€šè¿‡åç«¯ä»£ç†
2. **æŠ½è±¡æœåŠ¡å•†å±‚**: æ”¯æŒæ™ºè°±AIã€é˜¿é‡Œäº‘ç™¾ç‚¼ç­‰å¤šä¾›åº”å•†
3. **ç»Ÿä¸€é…ç½®ç®¡ç†**: æ¨¡å‹åˆ—è¡¨ã€é»˜è®¤æ¨¡å‹é›†ä¸­é…ç½®
4. **ä¿æŒå‘åå…¼å®¹**: ç°æœ‰åŠŸèƒ½ä¸å—å½±å“

### 1.3 æ•´åˆåŸåˆ™
- **ç”±ç®€åˆ°ç¹**: å…ˆéæµå¼ï¼Œåæµå¼ï¼›å…ˆå•æ¬¡ï¼Œåæ‰¹é‡
- **æ¸è¿›å¼**: æ¯ä¸ªé˜¶æ®µå¯ç‹¬ç«‹éƒ¨ç½²å’Œæµ‹è¯•
- **å¯å›æ»š**: ä¿ç•™åŸæœ‰ä»£ç ï¼Œé€šè¿‡å¼€å…³åˆ‡æ¢
- **ç‹¬ç«‹æµ‹è¯•å…ˆè¡Œ**: æ–°æœåŠ¡å•†å…ˆåœ¨ç‹¬ç«‹æµ‹è¯•é¡µé¢éªŒè¯ï¼Œå†æ•´åˆåˆ°ä¸»ç³»ç»Ÿ

---

## äºŒã€å¤šå‚å•†APIå·®å¼‚åˆ†æ

### 2.1 ğŸ‰ é‡å¤§å‘ç°ï¼šé˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒOpenAIå…¼å®¹æ¥å£

é˜¿é‡Œäº‘ç™¾ç‚¼æä¾›äº† **OpenAIå…¼å®¹æ¥å£**ï¼Œè¿™æ„å‘³ç€ï¼š
- å¯ä»¥ä½¿ç”¨ä¸æ™ºè°±AIå‡ ä¹ç›¸åŒçš„è°ƒç”¨æ–¹å¼
- å¤§å¹…é™ä½é€‚é…å¤æ‚åº¦
- å»ºè®®ä¼˜å…ˆä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼

**ä¸¤ç§è°ƒç”¨æ–¹å¼å¯¹æ¯”ï¼š**

| æ–¹å¼ | Base URL | ç‰¹ç‚¹ | æ¨èåº¦ |
|-----|---------|------|-------|
| **OpenAIå…¼å®¹æ¨¡å¼** | `https://dashscope.aliyuncs.com/compatible-mode/v1` | æ ¼å¼ä¸æ™ºè°±AIå‡ ä¹ä¸€è‡´ | â­â­â­ æ¨è |
| DashScopeåŸç”Ÿæ¨¡å¼ | `https://dashscope.aliyuncs.com/api/v1` | é˜¿é‡Œäº‘åŸç”Ÿæ ¼å¼ï¼Œéœ€é€‚é… | â­â­ å¤‡é€‰ |

### 2.2 è¯·æ±‚æ ¼å¼å·®å¼‚ï¼ˆOpenAIå…¼å®¹æ¨¡å¼ï¼‰

| ç‰¹æ€§ | æ™ºè°±AI (ZhipuAI) | é˜¿é‡Œäº‘ç™¾ç‚¼ (OpenAIå…¼å®¹) | å…¼å®¹æ€§ |
|-----|-----------------|------------------------|--------|
| è®¤è¯æ–¹å¼ | `Authorization: Bearer {api_key}` | `Authorization: Bearer {api_key}` | âœ… å®Œå…¨å…¼å®¹ |
| Base URL | `https://open.bigmodel.cn/api/paas/v4` | `https://dashscope.aliyuncs.com/compatible-mode/v1` | âš ï¸ éœ€é…ç½® |
| æ¨¡å‹å‚æ•° | `model: "glm-4-flash"` | `model: "qwen-plus"` | âš ï¸ éœ€æ˜ å°„ |
| æ¶ˆæ¯æ ¼å¼ | `messages: [{role, content}]` | `messages: [{role, content}]` | âœ… å®Œå…¨å…¼å®¹ |
| æµå¼å‚æ•° | `stream: true/false` | `stream: true/false` | âœ… å®Œå…¨å…¼å®¹ |
| æ¸©åº¦å‚æ•° | `temperature: 0.7` | `temperature: 0.7` | âœ… å®Œå…¨å…¼å®¹ |
| Tokenç»Ÿè®¡ | éœ€æ‰‹åŠ¨è§£æ | `stream_options: {include_usage: true}` | âš ï¸ æ ¼å¼ç•¥æœ‰ä¸åŒ |
| è”ç½‘æœç´¢ | `tools: [{type: "web_search"}]` | `extra_body: {"enable_search": True}` | âš ï¸ å‚æ•°æ ¼å¼ä¸åŒ |
| OCRæ¥å£ | `/layout_parsing` | éœ€ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ | âš ï¸ éœ€é€‚é… |

### 2.3 å“åº”æ ¼å¼å·®å¼‚ï¼ˆOpenAIå…¼å®¹æ¨¡å¼ï¼‰

| ç‰¹æ€§ | æ™ºè°±AI | é˜¿é‡Œäº‘ç™¾ç‚¼ (OpenAIå…¼å®¹) | å…¼å®¹æ€§ |
|-----|-------|------------------------|--------|
| éæµå¼å“åº” | `{choices: [{message: {content}}]}` | `{choices: [{message: {content}}]}` | âœ… å®Œå…¨å…¼å®¹ |
| æµå¼å“åº” | `data: {choices: [{delta: {content}}]}` | `data: {choices: [{delta: {content}}]}` | âœ… å®Œå…¨å…¼å®¹ |
| é”™è¯¯æ ¼å¼ | `{error: {message, type}}` | `{error: {message, type}}` | âœ… å®Œå…¨å…¼å®¹ |
| Tokenç»Ÿè®¡ | `usage: {total_tokens}` | `usage: {prompt_tokens, completion_tokens, total_tokens}` | âœ… å…¼å®¹ |

### 2.4 é˜¿é‡Œäº‘ç™¾ç‚¼ç‰¹æœ‰åŠŸèƒ½

#### 2.4.1 æ·±åº¦æ€è€ƒæ¨¡å¼

é˜¿é‡Œäº‘ç™¾ç‚¼æä¾›ä¸¤ç§æ€è€ƒæ¨¡å¼ï¼š

| æ¨¡å¼ | è¯´æ˜ | æ¨¡å‹ç¤ºä¾‹ | æ§åˆ¶æ–¹å¼ |
|-----|------|---------|---------|
| **æ··åˆæ€è€ƒæ¨¡å¼** | å¯å¼€å…³æ€è€ƒåŠŸèƒ½ | qwen-plus, qwen-turbo, qwen-max | `enable_thinking: true/false` |
| **ä»…æ€è€ƒæ¨¡å¼** | å§‹ç»ˆå¼€å¯æ€è€ƒ | qwq-plus, deepseek-r1, kimi-k2-thinking | æ— æ³•å…³é—­ |

**æ€è€ƒæ¨¡å¼å“åº”æ ¼å¼ï¼š**
```python
# æµå¼å“åº”ä¸­åŒ…å«ä¸¤ä¸ªå­—æ®µ
delta.reasoning_content  # æ€è€ƒè¿‡ç¨‹ï¼ˆå…ˆè¿”å›ï¼‰
delta.content            # å›å¤å†…å®¹ï¼ˆåè¿”å›ï¼‰
```

**å¼€å¯æ€è€ƒæ¨¡å¼ç¤ºä¾‹ï¼š**
```python
# Python SDK - é€šè¿‡ extra_body ä¼ å…¥
completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "ä½ æ˜¯è°"}],
    extra_body={"enable_thinking": True},  # å…³é”®å‚æ•°
    stream=True,
    stream_options={"include_usage": True}
)

# Node.js SDK - ä½œä¸ºé¡¶å±‚å‚æ•°
const stream = await openai.chat.completions.create({
    model: 'qwen-plus',
    messages,
    enable_thinking: true,  // ç›´æ¥ä¼ å…¥
    stream: true
});
```

**é™åˆ¶æ€è€ƒé•¿åº¦ï¼š**
```python
# thinking_budget å‚æ•°é™åˆ¶æ€è€ƒè¿‡ç¨‹æœ€å¤§Tokenæ•°
extra_body={
    "enable_thinking": True,
    "thinking_budget": 50  # æœ€å¤š50ä¸ªTokenç”¨äºæ€è€ƒ
}
```

#### 2.4.2 å¤šæ¨¡æ€èƒ½åŠ›

| æ¨¡å‹ | æ”¯æŒèƒ½åŠ› | è¯´æ˜ |
|-----|---------|------|
| qwen-vl-plus | å›¾ç‰‡ç†è§£ | æ”¯æŒå›¾ç‰‡URLæˆ–base64 |
| qwen-vl-max | å›¾ç‰‡ç†è§£ | æ›´å¼ºèƒ½åŠ› |
| qwen-long | æ–‡æ¡£è¾“å…¥ | æ”¯æŒé•¿æ–‡æ¡£ |

#### 2.4.3 ä¸Šä¸‹æ–‡ç¼“å­˜

- è‡ªåŠ¨å¼€å¯ï¼Œé™ä½å¤šè½®å¯¹è¯æˆæœ¬
- æ— éœ€é¢å¤–é…ç½®

#### 2.4.4 è”ç½‘æœç´¢

```python
# å¼€å¯è”ç½‘æœç´¢
extra_body={"enable_search": True}
```

**æ³¨æ„**: ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒè”ç½‘æœç´¢ï¼Œéœ€æ£€æŸ¥æ¨¡å‹æ–‡æ¡£ç¡®è®¤ã€‚

#### 2.4.5 ç»“æ„åŒ–è¾“å‡º

é˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒä¸¤ç§ç»“æ„åŒ–è¾“å‡ºæ¨¡å¼ï¼š

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ | ä¸¥æ ¼ç¨‹åº¦ |
|-----|------|---------|---------|
| **JSON Objectæ¨¡å¼** | è¾“å‡ºåˆæ³•JSONï¼Œä¸çº¦æŸç»“æ„ | çµæ´»æ•°æ®æŠ½å– | â­â­ å®½æ¾ |
| **JSON Schemaæ¨¡å¼** | ä¸¥æ ¼æŒ‰Schemaè¾“å‡º | ç¡®ä¿å­—æ®µç±»å‹å’Œç»“æ„ | â­â­â­ ä¸¥æ ¼ |

**JSON Objectæ¨¡å¼ç¤ºä¾‹ï¼š**
```python
completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "system", "content": "ä½ å¿…é¡»è¾“å‡ºJSONæ ¼å¼"},
        {"role": "user", "content": "è¯·åˆ—å‡ºä¸‰ç§ç¼–ç¨‹è¯­è¨€åŠå…¶ç‰¹ç‚¹"}
    ],
    response_format={"type": "json_object"}
)
# è¾“å‡º: {"languages": [{"name": "Python", "features": ["ç®€æ´", "æ˜“å­¦"]}, ...]}
```

**JSON Schemaæ¨¡å¼ç¤ºä¾‹ï¼ˆä¸“åˆ©æ•°æ®æŠ½å–ï¼‰ï¼š**
```python
patent_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "patent_info",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "applicant": {"type": "string"},
                "inventors": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "abstract": {"type": "string"},
                "claims_count": {"type": "integer"}
            },
            "required": ["title", "applicant", "inventors", "abstract"]
        }
    }
}

completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "user", "content": f"è¯·ä»ä»¥ä¸‹ä¸“åˆ©æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼š\n{patent_text}"}
    ],
    response_format=patent_schema
)
# è¾“å‡ºä¸¥æ ¼ç¬¦åˆSchemaå®šä¹‰çš„JSON
```

#### 2.4.6 æ‰¹é‡æ¨ç† (Batch API)

é˜¿é‡Œäº‘ç™¾ç‚¼æä¾›Batch APIï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å¼‚æ­¥å¤„ç†åœºæ™¯ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|-----|------|
| **æˆæœ¬ä¼˜åŠ¿** | 50%æŠ˜æ‰£ |
| **æ–‡ä»¶æ ¼å¼** | JSONLï¼ˆæ¯è¡Œä¸€ä¸ªè¯·æ±‚ï¼‰ |
| **è¯·æ±‚é™åˆ¶** | å•æ–‡ä»¶æœ€å¤š50,000è¯·æ±‚ |
| **å¤„ç†æ–¹å¼** | å¼‚æ­¥å¤„ç†ï¼Œæ”¯æŒOSSæ–‡ä»¶ |
| **é€‚ç”¨åœºæ™¯** | æ‰¹é‡ç¿»è¯‘ã€æ‰¹é‡åˆ†æã€å¤§è§„æ¨¡æ•°æ®å¤„ç† |

**JSONLæ–‡ä»¶æ ¼å¼ï¼š**
```jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen-plus", "messages": [{"role": "user", "content": "ç¿»è¯‘ï¼šHello"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen-plus", "messages": [{"role": "user", "content": "ç¿»è¯‘ï¼šWorld"}]}}
```

**æ‰¹é‡å¤„ç†æµç¨‹ï¼š**
```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# 1. ä¸Šä¼ æ‰¹é‡æ–‡ä»¶
batch_file = client.files.create(
    file=open("batch_requests.jsonl", "rb"),
    purpose="batch"
)

# 2. åˆ›å»ºæ‰¹é‡ä»»åŠ¡
batch_job = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h"
)

# 3. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
status = client.batches.retrieve(batch_job.id)
print(f"çŠ¶æ€: {status.status}")  # validatin/running/finalized

# 4. è·å–ç»“æœï¼ˆä»»åŠ¡å®Œæˆåï¼‰
if status.status == "finalized":
    result_file = client.files.content(status.output_file_id)
    results = result_file.read()
```

**æ‰¹é‡ç¿»è¯‘ç¤ºä¾‹ï¼ˆä¸“åˆ©åœºæ™¯ï¼‰ï¼š**
```python
import json

def create_batch_translation_requests(patents: list) -> str:
    """åˆ›å»ºæ‰¹é‡ç¿»è¯‘è¯·æ±‚æ–‡ä»¶"""
    requests = []
    for i, patent in enumerate(patents):
        requests.append({
            "custom_id": f"patent-{i}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "qwen-plus",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“åˆ©ç¿»è¯‘ä¸“å®¶"},
                    {"role": "user", "content": f"ç¿»è¯‘ä»¥ä¸‹ä¸“åˆ©æ‘˜è¦ä¸ºä¸­æ–‡ï¼š\n{patent['abstract']}"}
                ],
                "temperature": 0.3
            }
        })
    
    # å†™å…¥JSONLæ–‡ä»¶
    with open("batch_translate.jsonl", "w") as f:
        for req in requests:
            f.write(json.dumps(req) + "\n")
    
    return "batch_translate.jsonl"
```

### 2.5 éœ€è¦æŠ½è±¡çš„å…³é”®ç‚¹

```
1. æ¨¡å‹åç§°æ˜ å°„: ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ â†’ å®é™…APIæ¨¡å‹å
2. Base URLé…ç½®: ä¸åŒæœåŠ¡å•†ä½¿ç”¨ä¸åŒçš„base_url
3. ç‰¹æ®ŠåŠŸèƒ½é€‚é…: æ€è€ƒæ¨¡å¼ã€å¤šæ¨¡æ€ç­‰
4. åŠŸèƒ½æ”¯æŒæ£€æµ‹: è”ç½‘æœç´¢ã€OCRç­‰èƒ½åŠ›æ£€æµ‹
5. é”™è¯¯å¤„ç†: ç»Ÿä¸€é”™è¯¯ç å’Œæ¶ˆæ¯
```

---

## ä¸‰ã€åˆ†é˜¶æ®µæ•´åˆè®¡åˆ’

### ğŸ“Š æ•´ä½“è¿›åº¦æ¦‚è§ˆ

| é˜¶æ®µ | åç§° | çŠ¶æ€ | è¿›åº¦ |
|-----|------|------|------|
| Phase 0 | åŸºç¡€è®¾æ–½å‡†å¤‡ | âœ… å·²å®Œæˆ | 100% |
| Phase 1 | éæµå¼è°ƒç”¨ç»Ÿä¸€ | â³ æœªå¼€å§‹ | 0% |
| Phase 2 | ç®€å•æµå¼è°ƒç”¨ç»Ÿä¸€ | â³ æœªå¼€å§‹ | 0% |
| Phase 3 | åŠŸèƒ½å…«åç«¯ä»£ç†åŒ– | â³ æœªå¼€å§‹ | 0% |
| Phase 4 | æ‰¹é‡/å¼‚æ­¥è°ƒç”¨ç»Ÿä¸€ | â³ æœªå¼€å§‹ | 0% |
| **Phase 4.5** | **é˜¿é‡Œäº‘ç™¾ç‚¼ç‹¬ç«‹æµ‹è¯•é¡µé¢** | â³ æœªå¼€å§‹ | 0% |
| Phase 5 | å¤šæœåŠ¡å•†æ”¯æŒ | â³ æœªå¼€å§‹ | 0% |
| Phase 6 | é«˜çº§åŠŸèƒ½é€‚é… | â³ æœªå¼€å§‹ | 0% |

---

## Phase 0: åŸºç¡€è®¾æ–½å‡†å¤‡

**ç›®æ ‡**: åˆ›å»ºç»Ÿä¸€é…ç½®å’ŒæœåŠ¡å•†æŠ½è±¡å±‚åŸºç¡€ä»£ç 

**é¢„è®¡å·¥æ—¶**: 2-3å°æ—¶

**å®é™…å®Œæˆ**: 2026-02-28

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 0.1 | åˆ›å»º `config/providers.json` æœåŠ¡å•†é…ç½®æ–‡ä»¶ | âœ… | 2026-02-28 |
| 0.2 | åˆ›å»º `backend/services/llm/base_provider.py` æŠ½è±¡åŸºç±» | âœ… | 2026-02-28 |
| 0.3 | åˆ›å»º `backend/services/llm/zhipu_provider.py` æ™ºè°±å®ç° | âœ… | 2026-02-28 |
| 0.4 | åˆ›å»º `backend/services/llm/provider_factory.py` å·¥å‚ç±» | âœ… | 2026-02-28 |
| 0.5 | åˆ›å»º `backend/services/llm/aliyun_provider.py` é˜¿é‡Œäº‘å®ç° | âœ… | 2026-02-28 |
| 0.6 | åˆ›å»º `backend/services/llm/llm_service.py` ç»Ÿä¸€æœåŠ¡å±‚ | âœ… | 2026-02-28 |
| 0.7 | æ·»åŠ  `openai>=1.0.0` ä¾èµ– | âœ… | 2026-02-28 |

### è¯¦ç»†è¯´æ˜

#### 0.1 åˆ›å»ºæœåŠ¡å•†é…ç½®æ–‡ä»¶

**æ–‡ä»¶**: `config/providers.json`

```json
{
  "providers": {
    "zhipu": {
      "name": "æ™ºè°±AI",
      "api_base": "https://open.bigmodel.cn/api/paas/v4",
      "auth_header": "Authorization",
      "auth_prefix": "Bearer ",
      "models": ["glm-4-flash", "glm-4-plus", "glm-4-long"],
      "default_model": "glm-4-flash",
      "features": {
        "web_search": true,
        "ocr": true,
        "stream": true
      }
    },
    "aliyun": {
      "name": "é˜¿é‡Œäº‘ç™¾ç‚¼",
      "api_base": "https://dashscope.aliyuncs.com/api/v1",
      "auth_header": "Authorization",
      "auth_prefix": "Bearer ",
      "models": ["qwen-turbo", "qwen-plus", "qwen-max"],
      "default_model": "qwen-turbo",
      "features": {
        "web_search": false,
        "ocr": false,
        "stream": true
      },
      "enabled": false
    }
  },
  "default_provider": "zhipu"
}
```

#### 0.2 æŠ½è±¡åŸºç±»è®¾è®¡

**æ–‡ä»¶**: `backend/services/llm/base_provider.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, AsyncGenerator, Any

class BaseLLMProvider(ABC):
    """LLMæœåŠ¡å•†æŠ½è±¡åŸºç±»"""
    
    def __init__(self, api_key: str, config: Dict):
        self.api_key = api_key
        self.config = config
    
    @abstractmethod
    async def complete(
        self,
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        **kwargs
    ) -> Dict:
        """éæµå¼å®Œæˆ"""
        pass
    
    @abstractmethod
    async def stream(
        self,
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æµå¼å®Œæˆ"""
        pass
    
    @abstractmethod
    def get_model_list(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        pass
    
    @abstractmethod
    def parse_response(self, response: Any) -> Dict:
        """è§£æå“åº”ä¸ºç»Ÿä¸€æ ¼å¼"""
        pass
    
    @abstractmethod
    def parse_error(self, error: Any) -> Dict:
        """è§£æé”™è¯¯ä¸ºç»Ÿä¸€æ ¼å¼"""
        pass
```

### éªŒæ”¶æ ‡å‡†

- [ ] `providers.json` æ–‡ä»¶åˆ›å»ºå¹¶æ ¼å¼æ­£ç¡®
- [ ] `BaseLLMProvider` æŠ½è±¡ç±»å®šä¹‰å®Œæ•´
- [ ] `ZhipuProvider` å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•
- [ ] `ProviderFactory` å¯æ ¹æ®é…ç½®åˆ›å»ºProviderå®ä¾‹
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡

### æµ‹è¯•æ­¥éª¤

```bash
# 1. éªŒè¯é…ç½®æ–‡ä»¶åŠ è½½
python -c "import json; print(json.load(open('config/providers.json')))"

# 2. éªŒè¯Providerå®ä¾‹åŒ–
python -c "
from backend.services.llm.provider_factory import ProviderFactory
factory = ProviderFactory()
provider = factory.get_provider('zhipu', 'test-api-key')
print(f'Provider: {provider.__class__.__name__}')
print(f'Models: {provider.get_model_list()}')
"
```

---

## Phase 1: éæµå¼è°ƒç”¨ç»Ÿä¸€

**ç›®æ ‡**: å°†æ‰€æœ‰éæµå¼ä¸€æ¬¡æ€§è¾“å‡ºè°ƒç”¨ç»Ÿä¸€åˆ°æ–°æ¶æ„

**é¢„è®¡å·¥æ—¶**: 3-4å°æ—¶

**æ¶‰åŠåŠŸèƒ½**:
- åŠŸèƒ½äºŒï¼šä¸“åˆ©è§£è¯» (`/patent/analyze`)
- åŠŸèƒ½å››ï¼šæƒåˆ©è¦æ±‚AIç¿»è¯‘ (`/api/claims-analyzer/parse`)
- åŠŸèƒ½äº”ï¼šè¯´æ˜ä¹¦ç¿»è¯‘ (`/patent/translate`)
- åŠŸèƒ½ä¸ƒï¼šAIéƒ¨ä»¶æŠ½å– (`/drawing-marker/process` AIæ¨¡å¼)

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 1.1 | åˆ›å»º `backend/services/llm/llm_service.py` ç»Ÿä¸€æœåŠ¡å±‚ | â³ | - |
| 1.2 | é‡æ„ `patent.py` çš„ `analyze_patent` ä½¿ç”¨æ–°æœåŠ¡ | â³ | - |
| 1.3 | é‡æ„ `claims_analyzer.py` çš„ç¿»è¯‘åŠŸèƒ½ä½¿ç”¨æ–°æœåŠ¡ | â³ | - |
| 1.4 | é‡æ„ `translation_service.py` ä½¿ç”¨æ–°æœåŠ¡ | â³ | - |
| 1.5 | é‡æ„ `ai_component_extractor.py` ä½¿ç”¨æ–°æœåŠ¡ | â³ | - |
| 1.6 | æ·»åŠ åŠŸèƒ½å¼€å…³æ”¯æŒæ–°æ—§åˆ‡æ¢ | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 1.1 ç»Ÿä¸€æœåŠ¡å±‚

**æ–‡ä»¶**: `backend/services/llm/llm_service.py`

```python
class LLMService:
    """ç»Ÿä¸€çš„LLMè°ƒç”¨æœåŠ¡"""
    
    def __init__(self, provider_name: str = None):
        self.factory = ProviderFactory()
        self.provider_name = provider_name or self._get_default_provider()
    
    async def complete(
        self,
        messages: List[Dict],
        model: str = None,
        temperature: float = 0.7,
        provider: str = None,
        **kwargs
    ) -> Dict:
        """
        ç»Ÿä¸€çš„éæµå¼å®Œæˆæ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°(ç”¨æˆ·é€‰æ‹©çš„åç§°)
            temperature: æ¸©åº¦å‚æ•°
            provider: æŒ‡å®šæœåŠ¡å•†(å¯é€‰)
            
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„å“åº”:
            {
                "content": "å›å¤å†…å®¹",
                "model": "å®é™…ä½¿ç”¨çš„æ¨¡å‹",
                "usage": {"total_tokens": 100},
                "provider": "zhipu"
            }
        """
        provider_name = provider or self.provider_name
        provider_instance = self.factory.get_provider(provider_name)
        
        # æ¨¡å‹åç§°æ˜ å°„
        actual_model = self._map_model(model, provider_name)
        
        response = await provider_instance.complete(
            messages=messages,
            model=actual_model,
            temperature=temperature,
            **kwargs
        )
        
        return provider_instance.parse_response(response)
```

#### 1.6 åŠŸèƒ½å¼€å…³é…ç½®

**æ–‡ä»¶**: `backend/config/feature_flags.json`

```json
{
  "llm_unified": {
    "enabled": false,
    "description": "ä½¿ç”¨ç»Ÿä¸€LLMæœåŠ¡å±‚",
    "affects": ["patent/analyze", "claims-analyzer", "translate", "drawing-marker"]
  }
}
```

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰éæµå¼è°ƒç”¨ç‚¹éƒ½ä½¿ç”¨ `LLMService`
- [ ] å“åº”æ ¼å¼ç»Ÿä¸€ä¸ºæ ‡å‡†æ ¼å¼
- [ ] åŠŸèƒ½å¼€å…³å¯æ§åˆ¶æ–°æ—§åˆ‡æ¢
- [ ] åŸæœ‰åŠŸèƒ½ä¸å—å½±å“

### æµ‹è¯•æ­¥éª¤

```markdown
#### æµ‹è¯•1: ä¸“åˆ©è§£è¯»åŠŸèƒ½
1. æ‰“å¼€åŠŸèƒ½äºŒï¼Œè¾“å…¥ä¸“åˆ©å·çˆ¬å–ä¸“åˆ©
2. ç‚¹å‡»"AIè§£è¯»"æŒ‰é’®
3. éªŒè¯è¿”å›ç»“æœæ ¼å¼æ­£ç¡®
4. æ£€æŸ¥åç«¯æ—¥å¿—ç¡®è®¤ä½¿ç”¨æ–°æœåŠ¡

#### æµ‹è¯•2: æƒåˆ©è¦æ±‚ç¿»è¯‘
1. æ‰“å¼€åŠŸèƒ½å››ï¼Œä¸Šä¼ åŒ…å«éä¸­æ–‡æƒåˆ©è¦æ±‚çš„Excel
2. å¼€å¯AIç¿»è¯‘æ¨¡å¼
3. éªŒè¯ç¿»è¯‘ç»“æœæ­£ç¡®
4. æ£€æŸ¥åç«¯æ—¥å¿—ç¡®è®¤ä½¿ç”¨æ–°æœåŠ¡

#### æµ‹è¯•3: è¯´æ˜ä¹¦ç¿»è¯‘
1. æ‰“å¼€åŠŸèƒ½äº”ï¼Œçˆ¬å–è‹±æ–‡ä¸“åˆ©
2. ç‚¹å‡»ç¿»è¯‘è¯´æ˜ä¹¦
3. éªŒè¯ç¿»è¯‘ç»“æœæ­£ç¡®

#### æµ‹è¯•4: AIéƒ¨ä»¶æŠ½å–
1. æ‰“å¼€åŠŸèƒ½ä¸ƒï¼Œä¸Šä¼ é™„å›¾å’Œè¯´æ˜ä¹¦
2. å¼€å¯AIæ¨¡å¼
3. éªŒè¯éƒ¨ä»¶æŠ½å–ç»“æœæ­£ç¡®
```

---

## Phase 2: ç®€å•æµå¼è°ƒç”¨ç»Ÿä¸€

**ç›®æ ‡**: ç»Ÿä¸€ä¸å¸¦è”ç½‘æœç´¢çš„ç®€å•æµå¼å¯¹è¯

**é¢„è®¡å·¥æ—¶**: 3-4å°æ—¶

**æ¶‰åŠåŠŸèƒ½**:
- åŠŸèƒ½ä¸€ï¼šå³æ—¶å¯¹è¯ (ä¸å¸¦è”ç½‘æœç´¢)
- åŠŸèƒ½äºŒï¼šä¸“åˆ©å¯¹è¯

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 2.1 | åœ¨ `BaseLLMProvider` ä¸­å®ç°æµå¼æ–¹æ³• | â³ | - |
| 2.2 | åˆ›å»º `backend/routes/stream.py` ç»Ÿä¸€æµå¼è·¯ç”± | â³ | - |
| 2.3 | é‡æ„ `chat.py` çš„æµå¼å¤„ç†é€»è¾‘ | â³ | - |
| 2.4 | ç»Ÿä¸€SSEäº‹ä»¶æ ¼å¼ | â³ | - |
| 2.5 | å‰ç«¯é€‚é…ç»Ÿä¸€æµå¼æ¥å£ | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 2.4 ç»Ÿä¸€SSEäº‹ä»¶æ ¼å¼

æ— è®ºå“ªä¸ªæœåŠ¡å•†ï¼Œéƒ½è¾“å‡ºç»Ÿä¸€æ ¼å¼ï¼š

```
data: {"type": "content", "delta": "ä½ å¥½", "finish_reason": null}

data: {"type": "content", "delta": "ï¼Œæˆ‘æ˜¯", "finish_reason": null}

data: {"type": "done", "delta": "", "finish_reason": "stop", "usage": {"total_tokens": 100}}
```

#### 2.5 å‰ç«¯é€‚é…

ä¿®æ”¹ `js/core/api.js` çš„æµå¼å¤„ç†ï¼š

```javascript
// ç»Ÿä¸€çš„æµå¼è§£æ
for (const line of lines) {
    if (!line.startsWith('data: ')) continue;
    const data = JSON.parse(line.substring(6));
    
    switch (data.type) {
        case 'content':
            // å¤„ç†å†…å®¹å¢é‡
            fullContent += data.delta;
            break;
        case 'done':
            // å¤„ç†å®Œæˆ
            usageInfo = data.usage;
            break;
        case 'error':
            // å¤„ç†é”™è¯¯
            throw new Error(data.message);
    }
}
```

### éªŒæ”¶æ ‡å‡†

- [ ] æµå¼å¯¹è¯æ­£å¸¸å·¥ä½œ
- [ ] SSEäº‹ä»¶æ ¼å¼ç»Ÿä¸€
- [ ] å‰ç«¯æ­£ç¡®è§£ææ‰€æœ‰æœåŠ¡å•†å“åº”
- [ ] é”™è¯¯å¤„ç†ç»Ÿä¸€

### æµ‹è¯•æ­¥éª¤

```markdown
#### æµ‹è¯•1: å³æ—¶å¯¹è¯(éè”ç½‘)
1. æ‰“å¼€åŠŸèƒ½ä¸€
2. å…³é—­è”ç½‘æœç´¢å¼€å…³
3. å‘é€æ¶ˆæ¯ï¼ŒéªŒè¯æµå¼è¾“å‡ºæ­£å¸¸
4. æ£€æŸ¥å“åº”æ ¼å¼æ˜¯å¦ç¬¦åˆæ–°æ ‡å‡†

#### æµ‹è¯•2: ä¸“åˆ©å¯¹è¯
1. æ‰“å¼€åŠŸèƒ½äº”ï¼Œçˆ¬å–ä¸“åˆ©
2. ç‚¹å‡»"é—®ä¸€é—®"æ‰“å¼€å¯¹è¯çª—å£
3. å‘é€æ¶ˆæ¯ï¼ŒéªŒè¯æµå¼è¾“å‡ºæ­£å¸¸
4. æ£€æŸ¥Markdownæ¸²æŸ“æ­£ç¡®

#### æµ‹è¯•3: é”™è¯¯å¤„ç†
1. ä½¿ç”¨æ— æ•ˆAPI Key
2. éªŒè¯é”™è¯¯æ¶ˆæ¯æ ¼å¼ç»Ÿä¸€
3. éªŒè¯å‰ç«¯æ­£ç¡®æ˜¾ç¤ºé”™è¯¯
```

---

## Phase 3: åŠŸèƒ½å…«åç«¯ä»£ç†åŒ–

**ç›®æ ‡**: å°†åŠŸèƒ½å…«çš„å‰ç«¯ç›´è°ƒæ”¹ä¸ºåç«¯ä»£ç†

**é¢„è®¡å·¥æ—¶**: 4-5å°æ—¶

**æ¶‰åŠåŠŸèƒ½**:
- PDF-OCRè§£æ
- OCRå¯¹è¯
- OCRç¿»è¯‘

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 3.1 | åˆ›å»º `backend/routes/pdf_ocr.py` è·¯ç”±æ–‡ä»¶ | â³ | - |
| 3.2 | å®ç° `/pdf-ocr/parse` OCRè§£ææ¥å£ | â³ | - |
| 3.3 | å®ç° `/pdf-ocr/chat` OCRå¯¹è¯æ¥å£ | â³ | - |
| 3.4 | å®ç° `/pdf-ocr/translate` OCRç¿»è¯‘æ¥å£ | â³ | - |
| 3.5 | ä¿®æ”¹ `pdf-ocr-parser.js` è°ƒç”¨åç«¯æ¥å£ | â³ | - |
| 3.6 | ä¿®æ”¹ `pdf-ocr-chat.js` è°ƒç”¨åç«¯æ¥å£ | â³ | - |
| 3.7 | æ·»åŠ ç¼“å­˜æ”¯æŒ | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 3.2 OCRè§£ææ¥å£

**æ–‡ä»¶**: `backend/routes/pdf_ocr.py`

```python
@pdf_ocr_bp.route('/pdf-ocr/parse', methods=['POST'])
def parse_pdf():
    """
    PDF/å›¾ç‰‡OCRè§£æ
    
    Request:
        - file: base64ç¼–ç çš„æ–‡ä»¶æ•°æ®
        - model: OCRæ¨¡å‹(å¯é€‰)
    
    Response:
        {
            "pages": [...],
            "md_results": "...",
            "usage": {...}
        }
    """
    # è·å–å®¢æˆ·ç«¯
    client, error = get_zhipu_client()
    if error:
        return error
    
    # è°ƒç”¨GLM-OCR
    # ... å®ç°é€»è¾‘
```

#### 3.5 å‰ç«¯ä¿®æ”¹

**æ–‡ä»¶**: `js/modules/pdf-ocr/pdf-ocr-parser.js`

```javascript
// ä¿®æ”¹å‰: å‰ç«¯ç›´è°ƒ
async callGLMOCR(fileData, apiKey, settings) {
    const response = await fetch('https://open.bigmodel.cn/api/paas/v4/layout_parsing', {
        headers: { 'Authorization': `Bearer ${apiKey}` },
        body: JSON.stringify({ model: "glm-ocr", file: base64Data })
    });
}

// ä¿®æ”¹å: åç«¯ä»£ç†
async callGLMOCR(fileData, apiKey, settings) {
    const result = await apiCall('/pdf-ocr/parse', {
        file: base64Data,
        model: 'glm-ocr'
    }, 'POST', false);
    return result;
}
```

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰åŠŸèƒ½å…«è°ƒç”¨éƒ½é€šè¿‡åç«¯
- [ ] å‰ç«¯ä¸å†ç›´æ¥æš´éœ²API Key
- [ ] OCRè§£æç»“æœæ ¼å¼ä¸å˜
- [ ] å¯¹è¯å’Œç¿»è¯‘åŠŸèƒ½æ­£å¸¸

### æµ‹è¯•æ­¥éª¤

```markdown
#### æµ‹è¯•1: OCRè§£æ
1. æ‰“å¼€åŠŸèƒ½å…«ï¼Œä¸Šä¼ PDFæ–‡ä»¶
2. ç‚¹å‡»OCRè§£æ
3. éªŒè¯è§£æç»“æœæ­£ç¡®
4. æ£€æŸ¥ç½‘ç»œè¯·æ±‚ï¼Œç¡®è®¤è°ƒç”¨åç«¯æ¥å£

#### æµ‹è¯•2: OCRå¯¹è¯
1. åœ¨OCRç»“æœé¡µé¢ä½¿ç”¨å¯¹è¯åŠŸèƒ½
2. å‘é€å…³äºæ–‡æ¡£å†…å®¹çš„é—®é¢˜
3. éªŒè¯AIå›å¤æ­£ç¡®
4. æ£€æŸ¥ç½‘ç»œè¯·æ±‚ï¼Œç¡®è®¤è°ƒç”¨åç«¯æ¥å£

#### æµ‹è¯•3: OCRç¿»è¯‘
1. é€‰æ‹©OCRç»“æœä¸­çš„æ–‡æœ¬
2. ç‚¹å‡»ç¿»è¯‘æŒ‰é’®
3. éªŒè¯ç¿»è¯‘ç»“æœæ­£ç¡®
4. æ£€æŸ¥ç½‘ç»œè¯·æ±‚ï¼Œç¡®è®¤è°ƒç”¨åç«¯æ¥å£

#### æµ‹è¯•4: ç¼“å­˜éªŒè¯
1. è§£æåŒä¸€PDFä¸¤æ¬¡
2. éªŒè¯ç¬¬äºŒæ¬¡ä½¿ç”¨ç¼“å­˜
3. æ£€æŸ¥åç«¯æ—¥å¿—ç¡®è®¤ç¼“å­˜å‘½ä¸­
```

---

## Phase 4: æ‰¹é‡/å¼‚æ­¥è°ƒç”¨ç»Ÿä¸€

**ç›®æ ‡**: ç»Ÿä¸€æ‰¹é‡ç¿»è¯‘å’Œå¼‚æ­¥å¤„ç†é€»è¾‘

**é¢„è®¡å·¥æ—¶**: 3-4å°æ—¶

**æ¶‰åŠåŠŸèƒ½**:
- åŠŸèƒ½äº”ï¼šåŒæ—ä¸“åˆ©æƒåˆ©è¦æ±‚å¯¹æ¯”(æ‰¹é‡ç¿»è¯‘)
- åŠŸèƒ½äº”ï¼šè¯´æ˜ä¹¦ç¿»è¯‘(åˆ†æ®µç¿»è¯‘)
- åŠŸèƒ½ä¸ƒï¼šAIéƒ¨ä»¶æŠ½å–(å¼‚æ­¥å¤„ç†)

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 4.1 | åœ¨ `LLMService` ä¸­æ·»åŠ æ‰¹é‡å¤„ç†æ–¹æ³• | â³ | - |
| 4.2 | é‡æ„ `patent.py` çš„æ‰¹é‡ç¿»è¯‘é€»è¾‘ | â³ | - |
| 4.3 | ç»Ÿä¸€å¼‚æ­¥å¤„ç†æ¨¡å¼ | â³ | - |
| 4.4 | æ·»åŠ æ‰¹é‡ä»»åŠ¡çŠ¶æ€è¿½è¸ª | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 4.1 æ‰¹é‡å¤„ç†æ–¹æ³•

```python
class LLMService:
    async def batch_complete(
        self,
        tasks: List[Dict],
        model: str = None,
        max_concurrent: int = 5
    ) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
        
        Args:
            tasks: [{"id": 1, "messages": [...]}, ...]
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            [{"id": 1, "result": {...}}, ...]
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = []
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = {
                executor.submit(
                    self.complete_sync, 
                    task["messages"], 
                    model
                ): task["id"] 
                for task in tasks
            }
            
            for future in as_completed(futures):
                task_id = futures[future]
                try:
                    result = future.result()
                    results.append({"id": task_id, "result": result})
                except Exception as e:
                    results.append({"id": task_id, "error": str(e)})
        
        return results
```

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰¹é‡ç¿»è¯‘æ­£å¸¸å·¥ä½œ
- [ ] å¹¶å‘æ§åˆ¶æœ‰æ•ˆ
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ€§èƒ½ç¬¦åˆé¢„æœŸ

### æµ‹è¯•æ­¥éª¤

```markdown
#### æµ‹è¯•1: åŒæ—ä¸“åˆ©å¯¹æ¯”
1. æ‰“å¼€åŠŸèƒ½äº”ï¼Œçˆ¬å–ä¸“åˆ©
2. ç‚¹å‡»"åŒæ—å¯¹æ¯”"
3. é€‰æ‹©å¤šä¸ªåŒæ—ä¸“åˆ©
4. éªŒè¯æ‰¹é‡ç¿»è¯‘ç»“æœæ­£ç¡®
5. æ£€æŸ¥å¹¶å‘æ•°ä¸è¶…è¿‡é™åˆ¶

#### æµ‹è¯•2: å¤§é‡æ•°æ®å¤„ç†
1. ä¸Šä¼ åŒ…å«100+æƒåˆ©è¦æ±‚çš„Excel
2. æ‰§è¡Œæ‰¹é‡ç¿»è¯‘
3. éªŒè¯æ‰€æœ‰æ•°æ®æ­£ç¡®å¤„ç†
4. æ£€æŸ¥å†…å­˜ä½¿ç”¨æ­£å¸¸
```

---

## Phase 4.5: é˜¿é‡Œäº‘ç™¾ç‚¼ç‹¬ç«‹æµ‹è¯•é¡µé¢

**ç›®æ ‡**: åˆ›å»ºç‹¬ç«‹æµ‹è¯•é¡µé¢ï¼ŒéªŒè¯é˜¿é‡Œäº‘ç™¾ç‚¼APIå„é¡¹è°ƒç”¨æ–¹å¼

**é¢„è®¡å·¥æ—¶**: 3-4å°æ—¶

**é‡è¦æ€§**: â­â­â­ åœ¨æ•´åˆåˆ°ä¸»ç³»ç»Ÿå‰ï¼Œå¿…é¡»å…ˆéªŒè¯æ‰€æœ‰è°ƒç”¨æ–¹å¼

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 4.5.1 | åˆ›å»º `tests/aliyun_test_page.html` æµ‹è¯•é¡µé¢ | â³ | - |
| 4.5.2 | åˆ›å»º `tests/aliyun_test_backend.py` æµ‹è¯•åç«¯ | â³ | - |
| 4.5.3 | æµ‹è¯•éæµå¼è°ƒç”¨ | â³ | - |
| 4.5.4 | æµ‹è¯•æµå¼è°ƒç”¨ | â³ | - |
| 4.5.5 | æµ‹è¯•å¤šè½®å¯¹è¯ | â³ | - |
| 4.5.6 | æµ‹è¯•æ€è€ƒæ¨¡å¼ | â³ | - |
| 4.5.7 | æµ‹è¯•å¤šæ¨¡æ€(å›¾ç‰‡ç†è§£) | â³ | - |
| 4.5.8 | æµ‹è¯•Tokenç»Ÿè®¡ | â³ | - |
| 4.5.9 | æµ‹è¯•é”™è¯¯å¤„ç† | â³ | - |
| 4.5.10 | æ•´ç†æµ‹è¯•æŠ¥å‘Š | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 4.5.1 æµ‹è¯•é¡µé¢ç»“æ„

**æ–‡ä»¶**: `tests/aliyun_test_page.html`

```html
<!DOCTYPE html>
<html>
<head>
    <title>é˜¿é‡Œäº‘ç™¾ç‚¼APIæµ‹è¯•é¡µé¢</title>
</head>
<body>
    <h1>é˜¿é‡Œäº‘ç™¾ç‚¼APIæµ‹è¯•</h1>
    
    <!-- API Keyé…ç½® -->
    <section id="config">
        <h2>é…ç½®</h2>
        <label>API Key: <input type="password" id="api_key"></label>
        <label>æ¨¡å‹: 
            <select id="model">
                <option value="qwen-turbo">qwen-turbo</option>
                <option value="qwen-plus">qwen-plus</option>
                <option value="qwen-max">qwen-max</option>
            </select>
        </label>
    </section>
    
    <!-- æµ‹è¯•æ¨¡å— -->
    <section id="tests">
        <h2>æµ‹è¯•æ¨¡å—</h2>
        <button onclick="testNonStream()">æµ‹è¯•éæµå¼è°ƒç”¨</button>
        <button onclick="testStream()">æµ‹è¯•æµå¼è°ƒç”¨</button>
        <button onclick="testMultiRound()">æµ‹è¯•å¤šè½®å¯¹è¯</button>
        <button onclick="testThinking()">æµ‹è¯•æ€è€ƒæ¨¡å¼</button>
        <button onclick="testMultimodal()">æµ‹è¯•å¤šæ¨¡æ€</button>
    </section>
    
    <!-- ç»“æœæ˜¾ç¤º -->
    <section id="results">
        <h2>æµ‹è¯•ç»“æœ</h2>
        <div id="output"></div>
    </section>
</body>
</html>
```

#### 4.5.2 æµ‹è¯•åç«¯ç»“æ„

**æ–‡ä»¶**: `tests/aliyun_test_backend.py`

```python
from flask import Blueprint, request, Response, stream_with_context
from openai import OpenAI
import json

aliyun_test_bp = Blueprint('aliyun_test', __name__)

ALIYUN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

def get_client(api_key):
    return OpenAI(
        api_key=api_key,
        base_url=ALIYUN_BASE_URL
    )

@aliyun_test_bp.route('/test/aliyun/complete', methods=['POST'])
def test_complete():
    """æµ‹è¯•éæµå¼è°ƒç”¨"""
    data = request.json
    client = get_client(data['api_key'])
    
    response = client.chat.completions.create(
        model=data['model'],
        messages=data['messages'],
        temperature=data.get('temperature', 0.7)
    )
    
    return {
        "content": response.choices[0].message.content,
        "usage": response.usage.model_dump(),
        "model": response.model
    }

@aliyun_test_bp.route('/test/aliyun/stream', methods=['POST'])
def test_stream():
    """æµ‹è¯•æµå¼è°ƒç”¨"""
    data = request.json
    client = get_client(data['api_key'])
    
    def generate():
        stream = client.chat.completions.create(
            model=data['model'],
            messages=data['messages'],
            temperature=data.get('temperature', 0.7),
            stream=True,
            stream_options={"include_usage": True}
        )
        
        for chunk in stream:
            if chunk.choices:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield f"data: {json.dumps({'type': 'content', 'delta': delta.content})}\n\n"
            elif chunk.usage:
                yield f"data: {json.dumps({'type': 'done', 'usage': chunk.usage.model_dump()})}\n\n"
        
        yield "data: [DONE]\n\n"
    
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

@aliyun_test_bp.route('/test/aliyun/thinking', methods=['POST'])
def test_thinking():
    """æµ‹è¯•æ€è€ƒæ¨¡å¼"""
    data = request.json
    client = get_client(data['api_key'])
    
    def generate():
        stream = client.chat.completions.create(
            model=data['model'],
            messages=data['messages'],
            stream=True,
            extra_body={"enable_thinking": True}
        )
        
        for chunk in stream:
            delta = chunk.choices[0].delta if chunk.choices else None
            if delta:
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    yield f"data: {json.dumps({'type': 'reasoning', 'delta': delta.reasoning_content})}\n\n"
                elif delta.content:
                    yield f"data: {json.dumps({'type': 'content', 'delta': delta.content})}\n\n"
        
        yield "data: [DONE]\n\n"
    
    return Response(stream_with_context(generate()), mimetype='text/event-stream')
```

### æµ‹è¯•ç”¨ä¾‹æ¸…å•

#### æµ‹è¯•1: éæµå¼è°ƒç”¨
```python
# è¯·æ±‚
{
    "model": "qwen-plus",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]
}

# é¢„æœŸå“åº”
{
    "content": "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®...",
    "usage": {"prompt_tokens": 15, "completion_tokens": 50, "total_tokens": 65},
    "model": "qwen-plus"
}
```

#### æµ‹è¯•2: æµå¼è°ƒç”¨
```python
# éªŒè¯ç‚¹
- SSEæ ¼å¼æ­£ç¡®
- deltaå†…å®¹é€’å¢
- usageä¿¡æ¯åœ¨æœ€åè¿”å›
- [DONE]æ ‡è®°å­˜åœ¨
```

#### æµ‹è¯•3: å¤šè½®å¯¹è¯
```python
# éªŒè¯ç‚¹
- messagesæ•°ç»„æ­£ç¡®ä¼ é€’
- ä¸Šä¸‹æ–‡ä¿æŒ
- Tokenæ¶ˆè€—éšè½®æ¬¡å¢åŠ 
```

#### æµ‹è¯•4: æ€è€ƒæ¨¡å¼
```python
# æµ‹è¯•æ··åˆæ€è€ƒæ¨¡å¼
# è¯·æ±‚
{
    "model": "qwen-plus",
    "messages": [{"role": "user", "content": "è¯·è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}],
    "enable_thinking": true,
    "stream": true
}

# éªŒè¯ç‚¹
- reasoning_content å…ˆè¿”å›ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
- content åè¿”å›ï¼ˆæœ€ç»ˆå›å¤ï¼‰
- ä¸¤è€…åˆ†ç¦»æ­£ç¡®
- æ€è€ƒè¿‡ç¨‹å†…å®¹åˆç†
- å¯é€šè¿‡ enable_thinking: false å…³é—­

# æµ‹è¯•ä»…æ€è€ƒæ¨¡å¼æ¨¡å‹
# è¯·æ±‚
{
    "model": "qwq-plus",  # ä»…æ€è€ƒæ¨¡å¼æ¨¡å‹
    "messages": [{"role": "user", "content": "1+1ç­‰äºå‡ ï¼Ÿ"}],
    "stream": true
}

# éªŒè¯ç‚¹
- æ— éœ€ enable_thinking å‚æ•°
- å§‹ç»ˆè¿”å›æ€è€ƒè¿‡ç¨‹
- æ— æ³•å…³é—­æ€è€ƒæ¨¡å¼
```

#### æµ‹è¯•5: æ€è€ƒé•¿åº¦é™åˆ¶
```python
# æµ‹è¯• thinking_budget å‚æ•°
{
    "model": "qwen-plus",
    "messages": [{"role": "user", "content": "ä½ æ˜¯è°"}],
    "enable_thinking": true,
    "thinking_budget": 50,  # é™åˆ¶æ€è€ƒè¿‡ç¨‹æœ€å¤š50 Token
    "stream": true
}

# éªŒè¯ç‚¹
- æ€è€ƒè¿‡ç¨‹Tokenæ•°ä¸è¶…è¿‡é™åˆ¶
- è¾¾åˆ°é™åˆ¶åç«‹å³å¼€å§‹å›å¤
- å›å¤å†…å®¹ä»ç„¶å®Œæ•´
```

#### æµ‹è¯•6: é”™è¯¯å¤„ç†
```python
# æµ‹è¯•åœºæ™¯
- æ— æ•ˆAPI Key
- æ¨¡å‹ä¸å­˜åœ¨
- è¯·æ±‚è¶…æ—¶
- Tokenè¶…é™
```

### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] éæµå¼è°ƒç”¨æ­£å¸¸è¿”å›
- [ ] æµå¼è°ƒç”¨SSEæ ¼å¼æ­£ç¡®
- [ ] å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡æ­£ç¡®
- [ ] æ€è€ƒæ¨¡å¼åˆ†ç¦»æ­£ç¡®
- [ ] é”™è¯¯å¤„ç†å‹å¥½
- [ ] æµ‹è¯•æŠ¥å‘Šå®Œæˆ

### æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

```markdown
# é˜¿é‡Œäº‘ç™¾ç‚¼APIæµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¥æœŸ**: YYYY-MM-DD
**æµ‹è¯•äººå‘˜**: 
**API KeyçŠ¶æ€**: æœ‰æ•ˆ/æ— æ•ˆ

## æµ‹è¯•ç»“æœæ±‡æ€»

| æµ‹è¯•é¡¹ | çŠ¶æ€ | å¤‡æ³¨ |
|-------|------|------|
| éæµå¼è°ƒç”¨ | âœ…/âŒ | |
| æµå¼è°ƒç”¨ | âœ…/âŒ | |
| å¤šè½®å¯¹è¯ | âœ…/âŒ | |
| æ€è€ƒæ¨¡å¼ | âœ…/âŒ | |
| å¤šæ¨¡æ€ | âœ…/âŒ | |
| Tokenç»Ÿè®¡ | âœ…/âŒ | |
| é”™è¯¯å¤„ç† | âœ…/âŒ | |

## å‘ç°çš„é—®é¢˜

1. é—®é¢˜æè¿°
   - å½±å“ç¨‹åº¦: é«˜/ä¸­/ä½
   - è§£å†³æ–¹æ¡ˆå»ºè®®: 

## ç»“è®º

- [ ] é€šè¿‡ï¼Œå¯è¿›å…¥Phase 5æ•´åˆ
- [ ] éœ€è§£å†³é—®é¢˜åå†æ•´åˆ
```

---

## Phase 5: å¤šæœåŠ¡å•†æ”¯æŒ

**ç›®æ ‡**: é›†æˆé˜¿é‡Œäº‘ç™¾ç‚¼ç­‰æ–°æœåŠ¡å•†

**é¢„è®¡å·¥æ—¶**: 3-4å°æ—¶ï¼ˆå› OpenAIå…¼å®¹æ¥å£å¤§å¹…ç®€åŒ–ï¼‰

**å‰ç½®æ¡ä»¶**: Phase 4.5æµ‹è¯•é€šè¿‡

### ğŸ‰ é‡è¦ç®€åŒ–

ç”±äºé˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼Œå®ç°å˜å¾—éå¸¸ç®€å•ï¼š
- æ— éœ€å¤æ‚çš„æ ¼å¼è½¬æ¢
- åªéœ€é…ç½®ä¸åŒçš„base_urlå’Œæ¨¡å‹åç§°
- å¯å¤ç”¨OpenAI SDK

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 5.1 | åˆ›å»º `backend/services/llm/aliyun_provider.py` | â³ | - |
| 5.2 | æ›´æ–° `providers.json` é…ç½® | â³ | - |
| 5.3 | å‰ç«¯æ·»åŠ æœåŠ¡å•†é€‰æ‹©UI | â³ | - |
| 5.4 | æ·»åŠ æœåŠ¡å•†å¥åº·æ£€æŸ¥ | â³ | - |
| 5.5 | æ•´åˆæµ‹è¯• | â³ | - |

### è¯¦ç»†è¯´æ˜

#### 5.1 é˜¿é‡Œäº‘Providerå®ç°ï¼ˆæç®€ç‰ˆï¼‰

**æ–‡ä»¶**: `backend/services/llm/aliyun_provider.py`

```python
from openai import OpenAI
from .base_provider import BaseLLMProvider
from typing import Dict, List, AsyncGenerator, Any

class AliyunProvider(BaseLLMProvider):
    """é˜¿é‡Œäº‘ç™¾ç‚¼Provider - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
    
    def __init__(self, api_key: str, config: Dict):
        super().__init__(api_key, config)
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
    
    def complete(
        self,
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        **kwargs
    ) -> Dict:
        """éæµå¼å®Œæˆ - ä¸æ™ºè°±AIæ ¼å¼å®Œå…¨å…¼å®¹"""
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            **kwargs
        )
        return self.parse_response(response)
    
    def stream(
        self,
        messages: List[Dict],
        model: str,
        temperature: float = 0.7,
        **kwargs
    ):
        """æµå¼å®Œæˆ - ä¸æ™ºè°±AIæ ¼å¼å®Œå…¨å…¼å®¹"""
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            stream=True,
            stream_options={"include_usage": True},
            **kwargs
        )
    
    def get_model_list(self) -> List[str]:
        return ["qwen-turbo", "qwen-plus", "qwen-max", "qwen-long"]
    
    def parse_response(self, response: Any) -> Dict:
        """è§£æå“åº” - æ ¼å¼ä¸æ™ºè°±AIä¸€è‡´"""
        return {
            "content": response.choices[0].message.content,
            "model": response.model,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            "provider": "aliyun"
        }
    
    def parse_error(self, error: Any) -> Dict:
        return {
            "code": getattr(error, 'code', 'unknown'),
            "message": str(error),
            "provider": "aliyun"
        }
    
    def get_features(self) -> Dict:
        """è¿”å›æ”¯æŒçš„åŠŸèƒ½"""
        return {
            "web_search": False,  # æš‚ä¸æ”¯æŒè”ç½‘æœç´¢
            "ocr": False,         # éœ€ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹
            "stream": True,
            "thinking": True,     # æ”¯æŒæ€è€ƒæ¨¡å¼
            "multimodal": True    # æ”¯æŒå¤šæ¨¡æ€
        }
```

#### 5.2 æ›´æ–°æœåŠ¡å•†é…ç½®

**æ–‡ä»¶**: `config/providers.json`

```json
{
  "providers": {
    "zhipu": {
      "name": "æ™ºè°±AI",
      "api_base": "https://open.bigmodel.cn/api/paas/v4",
      "sdk_class": "zhipuai.ZhipuAI",
      "models": [
        {"id": "glm-4-flash", "name": "GLM-4-Flash", "type": "chat"},
        {"id": "glm-4-plus", "name": "GLM-4-Plus", "type": "chat"},
        {"id": "glm-4-long", "name": "GLM-4-Long", "type": "chat"},
        {"id": "glm-ocr", "name": "GLM-OCR", "type": "ocr"}
      ],
      "default_model": "glm-4-flash",
      "features": {
        "web_search": true,
        "ocr": true,
        "stream": true,
        "thinking": false,
        "multimodal": false
      }
    },
    "aliyun": {
      "name": "é˜¿é‡Œäº‘ç™¾ç‚¼",
      "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
      "sdk_class": "openai.OpenAI",
      "models": [
        {"id": "qwen-turbo", "name": "é€šä¹‰åƒé—®-Turbo", "type": "chat"},
        {"id": "qwen-plus", "name": "é€šä¹‰åƒé—®-Plus", "type": "chat"},
        {"id": "qwen-max", "name": "é€šä¹‰åƒé—®-Max", "type": "chat"},
        {"id": "qwen-long", "name": "é€šä¹‰åƒé—®-Long", "type": "chat"},
        {"id": "qwen-vl-plus", "name": "é€šä¹‰åƒé—®-VL-Plus", "type": "multimodal"}
      ],
      "default_model": "qwen-plus",
      "features": {
        "web_search": false,
        "ocr": false,
        "stream": true,
        "thinking": true,
        "multimodal": true
      },
      "enabled": true
    }
  },
  "default_provider": "zhipu"
}
```

#### 5.3 å‰ç«¯æœåŠ¡å•†é€‰æ‹©UI

**ä¿®æ”¹æ–‡ä»¶**: `js/core/settings.js` æˆ–ç›¸å…³è®¾ç½®æ¨¡å—

```javascript
// æœåŠ¡å•†é€‰æ‹©å™¨
function renderProviderSelector() {
    const providers = [
        { id: 'zhipu', name: 'æ™ºè°±AI', features: ['è”ç½‘æœç´¢', 'OCR'] },
        { id: 'aliyun', name: 'é˜¿é‡Œäº‘ç™¾ç‚¼', features: ['æ€è€ƒæ¨¡å¼', 'å¤šæ¨¡æ€'] }
    ];
    
    return `
        <div class="provider-selector">
            <label>æœåŠ¡å•†</label>
            <select id="provider_select" onchange="switchProvider(this.value)">
                ${providers.map(p => `
                    <option value="${p.id}">${p.name}</option>
                `).join('')}
            </select>
            <div class="provider-features">
                æ”¯æŒåŠŸèƒ½: ${providers.find(p => p.id === currentProvider).features.join(', ')}
            </div>
        </div>
    `;
}

// åˆ‡æ¢æœåŠ¡å•†
async function switchProvider(providerId) {
    // æ›´æ–°æ¨¡å‹åˆ—è¡¨
    const models = await fetchModels(providerId);
    updateModelSelector(models);
    
    // æ›´æ–°åŠŸèƒ½å¼€å…³
    updateFeatureToggles(providerId);
    
    // ä¿å­˜åˆ°localStorage
    localStorage.setItem('selectedProvider', providerId);
}
```

#### 5.4 æœåŠ¡å•†å¥åº·æ£€æŸ¥

**æ–‡ä»¶**: `backend/routes/health.py`

```python
@health_bp.route('/health/llm/<provider>', methods=['GET'])
def check_llm_health(provider):
    """æ£€æŸ¥LLMæœåŠ¡å•†å¥åº·çŠ¶æ€"""
    try:
        if provider == 'zhipu':
            client = get_zhipu_client()
            # å‘é€ç®€å•è¯·æ±‚æµ‹è¯•
            response = client.chat.completions.create(
                model="glm-4-flash",
                messages=[{"role": "user", "content": "hi"}],
                max_tokens=1
            )
        elif provider == 'aliyun':
            client = get_aliyun_client()
            response = client.chat.completions.create(
                model="qwen-turbo",
                messages=[{"role": "user", "content": "hi"}],
                max_tokens=1
            )
        
        return {
            "status": "healthy",
            "provider": provider,
            "latency_ms": response.response_ms if hasattr(response, 'response_ms') else None
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "provider": provider,
            "error": str(e)
        }, 500
```

### éªŒæ”¶æ ‡å‡†

- [ ] é˜¿é‡Œäº‘æ¨¡å‹å¯æ­£å¸¸è°ƒç”¨
- [ ] å“åº”æ ¼å¼ä¸æ™ºè°±ä¸€è‡´
- [ ] å‰ç«¯å¯åˆ‡æ¢æœåŠ¡å•†
- [ ] åŠŸèƒ½æ”¯æŒæ£€æµ‹æ­£ç¡®
- [ ] ä¸æ”¯æŒçš„åŠŸèƒ½æœ‰å‹å¥½æç¤º
- [ ] é”™è¯¯å¤„ç†ç»Ÿä¸€

### æµ‹è¯•æ­¥éª¤

```markdown
#### æµ‹è¯•1: æœåŠ¡å•†åˆ‡æ¢
1. æ‰“å¼€è®¾ç½®é¡µé¢
2. é€‰æ‹©é˜¿é‡Œäº‘æœåŠ¡å•†
3. è¾“å…¥é˜¿é‡Œäº‘API Key
4. éªŒè¯æ¨¡å‹åˆ—è¡¨æ›´æ–°ä¸ºqwenç³»åˆ—

#### æµ‹è¯•2: å¯¹è¯åŠŸèƒ½
1. ä½¿ç”¨é˜¿é‡Œäº‘æ¨¡å‹è¿›è¡Œå¯¹è¯
2. éªŒè¯å“åº”æ­£ç¡®
3. éªŒè¯Tokenç»Ÿè®¡æ­£ç¡®

#### æµ‹è¯•3: åŠŸèƒ½æ£€æµ‹
1. åˆ‡æ¢åˆ°é˜¿é‡Œäº‘
2. å°è¯•å¼€å¯è”ç½‘æœç´¢
3. éªŒè¯æç¤º"è¯¥æœåŠ¡å•†æš‚ä¸æ”¯æŒè”ç½‘æœç´¢"

#### æµ‹è¯•4: æ€è€ƒæ¨¡å¼(é˜¿é‡Œäº‘ç‰¹æœ‰)
1. ä½¿ç”¨é˜¿é‡Œäº‘æ¨¡å‹
2. å¼€å¯æ€è€ƒæ¨¡å¼
3. éªŒè¯æ€è€ƒè¿‡ç¨‹å’Œå›å¤åˆ†ç¦»æ˜¾ç¤º
```

---

## Phase 6: é«˜çº§åŠŸèƒ½é€‚é…

**ç›®æ ‡**: é€‚é…è”ç½‘æœç´¢ã€OCRç­‰é«˜çº§åŠŸèƒ½

**é¢„è®¡å·¥æ—¶**: 4-5å°æ—¶

### ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | çŠ¶æ€ | å®Œæˆæ—¶é—´ |
|---|------|------|---------|
| 6.1 | æŠ½è±¡è”ç½‘æœç´¢æ¥å£ | â³ | - |
| 6.2 | å®ç°æ™ºè°±è”ç½‘æœç´¢é€‚é… | â³ | - |
| 6.3 | ç ”ç©¶é˜¿é‡Œäº‘è”ç½‘æœç´¢èƒ½åŠ› | â³ | - |
| 6.4 | æŠ½è±¡OCRæ¥å£ | â³ | - |
| 6.5 | å®ç°å¤šæœåŠ¡å•†OCRé€‚é… | â³ | - |

### éªŒæ”¶æ ‡å‡†

- [ ] è”ç½‘æœç´¢åŠŸèƒ½æ­£å¸¸
- [ ] OCRåŠŸèƒ½æ­£å¸¸
- [ ] ä¸æ”¯æŒçš„åŠŸèƒ½æœ‰å‹å¥½æç¤º

---

## å››ã€è¿›åº¦è¿½è¸ªæœºåˆ¶

### 4.1 æ–‡æ¡£æ›´æ–°è§„åˆ™

æ¯æ¬¡å®Œæˆä»»åŠ¡åï¼Œæ›´æ–°æœ¬æ–‡æ¡£ï¼š
1. å°†ä»»åŠ¡çŠ¶æ€ä» â³ æ”¹ä¸º âœ…
2. å¡«å†™å®Œæˆæ—¶é—´
3. æ·»åŠ å¤‡æ³¨è¯´æ˜

### 4.2 ä»£ç æäº¤è§„èŒƒ

```bash
# æäº¤æ ¼å¼
git commit -m "feat(llm): Phase X.X - ä»»åŠ¡æè¿°

- è¯¦ç»†å˜æ›´1
- è¯¦ç»†å˜æ›´2

Refs: #issue_number"
```

### 4.3 æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

æ¯ä¸ªPhaseå®Œæˆåï¼Œåˆ›å»ºæµ‹è¯•æŠ¥å‘Šï¼š

```markdown
# Phase X æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¥æœŸ**: YYYY-MM-DD
**æµ‹è¯•äººå‘˜**: 
**æµ‹è¯•ç¯å¢ƒ**: 

## æµ‹è¯•ç»“æœ

| æµ‹è¯•é¡¹ | é¢„æœŸç»“æœ | å®é™…ç»“æœ | çŠ¶æ€ |
|-------|---------|---------|------|
| ... | ... | ... | âœ…/âŒ |

## å‘ç°çš„é—®é¢˜

1. é—®é¢˜æè¿°
   - ä¸¥é‡ç¨‹åº¦: é«˜/ä¸­/ä½
   - å¤ç°æ­¥éª¤: 
   - è§£å†³æ–¹æ¡ˆ: 

## ç»“è®º

- [ ] é€šè¿‡ï¼Œå¯è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
- [ ] éœ€ä¿®å¤é—®é¢˜åé‡æµ‹
```

---

## äº”ã€å›æ»šæ–¹æ¡ˆ

### 5.1 åŠŸèƒ½å¼€å…³å›æ»š

```json
// config/feature_flags.json
{
  "llm_unified": {
    "enabled": false  // è®¾ä¸ºfalseå³å¯å›æ»š
  }
}
```

### 5.2 ä»£ç åˆ†æ”¯ç­–ç•¥

```
main
  â””â”€â”€ feature/llm-unified
        â”œâ”€â”€ phase-0-infrastructure
        â”œâ”€â”€ phase-1-non-streaming
        â”œâ”€â”€ phase-2-simple-streaming
        â”œâ”€â”€ phase-3-pdf-ocr-proxy
        â”œâ”€â”€ phase-4-batch-async
        â”œâ”€â”€ phase-5-multi-provider
        â””â”€â”€ phase-6-advanced-features
```

æ¯ä¸ªPhaseå®Œæˆååˆå¹¶åˆ° `feature/llm-unified`ï¼Œæ•´ä½“æµ‹è¯•é€šè¿‡ååˆå¹¶åˆ° `main`ã€‚

---

## å…­ã€é£é™©ä¸åº”å¯¹

| é£é™© | å¯èƒ½æ€§ | å½±å“ | åº”å¯¹æªæ–½ |
|-----|-------|------|---------|
| é˜¿é‡Œäº‘APIæ ¼å¼å·®å¼‚å¤§ | ä¸­ | é«˜ | å……åˆ†ç ”ç©¶æ–‡æ¡£ï¼Œé¢„ç•™é€‚é…æ—¶é—´ |
| æµå¼å“åº”æ ¼å¼ä¸å…¼å®¹ | ä¸­ | é«˜ | ç»Ÿä¸€SSEæ ¼å¼ï¼Œå‰ç«¯é€‚é… |
| æ€§èƒ½ä¸‹é™ | ä½ | ä¸­ | æ€§èƒ½æµ‹è¯•ï¼Œä¼˜åŒ–å¹¶å‘ |
| ç°æœ‰åŠŸèƒ½å›å½’ | ä¸­ | é«˜ | å®Œæ•´å›å½’æµ‹è¯• |

---

## ä¸ƒã€é™„å½•

### A. ç›¸å…³æ–‡ä»¶æ¸…å•

```
config/
â”œâ”€â”€ providers.json          # æœåŠ¡å•†é…ç½®
â”œâ”€â”€ models.json            # æ¨¡å‹é…ç½®(æ›´æ–°)
â””â”€â”€ feature_flags.json     # åŠŸèƒ½å¼€å…³(æ–°å¢)

backend/services/llm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_provider.py       # æŠ½è±¡åŸºç±»
â”œâ”€â”€ zhipu_provider.py      # æ™ºè°±å®ç°
â”œâ”€â”€ aliyun_provider.py     # é˜¿é‡Œäº‘å®ç°
â”œâ”€â”€ provider_factory.py    # å·¥å‚ç±»
â””â”€â”€ llm_service.py         # ç»Ÿä¸€æœåŠ¡å±‚

backend/routes/
â”œâ”€â”€ stream.py              # ç»Ÿä¸€æµå¼è·¯ç”±(æ–°å¢)
â”œâ”€â”€ pdf_ocr.py             # PDF-OCRè·¯ç”±(æ–°å¢)
â””â”€â”€ health.py              # å¥åº·æ£€æŸ¥(æ–°å¢)

tests/
â”œâ”€â”€ aliyun_test_page.html  # é˜¿é‡Œäº‘æµ‹è¯•é¡µé¢(æ–°å¢)
â””â”€â”€ aliyun_test_backend.py # é˜¿é‡Œäº‘æµ‹è¯•åç«¯(æ–°å¢)

js/core/
â”œâ”€â”€ api.js                 # APIè°ƒç”¨(æ›´æ–°)
â””â”€â”€ settings.js            # è®¾ç½®æ¨¡å—(æ›´æ–°)
```

### B. å‚è€ƒæ–‡æ¡£

**æ™ºè°±AI:**
- APIæ–‡æ¡£: https://open.bigmodel.cn/dev/api
- SDKæ–‡æ¡£: https://open.bigmodel.cn/dev/api#sdk

**é˜¿é‡Œäº‘ç™¾ç‚¼:**
- å¤šè½®å¯¹è¯: https://help.aliyun.com/zh/model-studio/multi-round-conversation
- æµå¼è¾“å‡º: https://help.aliyun.com/zh/model-studio/user-guide/streaming
- OpenAIå…¼å®¹: https://help.aliyun.com/zh/model-studio/compatibility-of-openai-with-dashscope
- æ¨¡å‹åˆ—è¡¨: https://help.aliyun.com/zh/model-studio/getting-started/models
- è·å–API Key: https://help.aliyun.com/zh/model-studio/get-api-key

### C. é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹ID | åç§° | ä¸Šä¸‹æ–‡é•¿åº¦ | ç‰¹ç‚¹ |
|-------|------|----------|------|
| qwen-turbo | é€šä¹‰åƒé—®-Turbo | 128K | å¿«é€Ÿå“åº”ï¼Œé€‚åˆç®€å•ä»»åŠ¡ |
| qwen-plus | é€šä¹‰åƒé—®-Plus | 128K | å¹³è¡¡æ€§èƒ½ä¸æˆæœ¬ |
| qwen-max | é€šä¹‰åƒé—®-Max | 32K | æœ€å¼ºèƒ½åŠ›ï¼Œå¤æ‚ä»»åŠ¡ |
| qwen-long | é€šä¹‰åƒé—®-Long | 1M | è¶…é•¿ä¸Šä¸‹æ–‡ |
| qwen-vl-plus | é€šä¹‰åƒé—®-VL-Plus | 32K | å¤šæ¨¡æ€ï¼Œæ”¯æŒå›¾ç‰‡ |
| qwen-vl-max | é€šä¹‰åƒé—®-VL-Max | 32K | å¤šæ¨¡æ€ï¼Œæ›´å¼ºèƒ½åŠ› |

### D. å…³é”®ä»£ç ç¤ºä¾‹

#### D.1 é˜¿é‡Œäº‘ç™¾ç‚¼æµå¼è°ƒç”¨ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# æµå¼è°ƒç”¨
stream = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    stream=True,
    stream_options={"include_usage": True}  # è·å–Tokenç»Ÿè®¡
)

for chunk in stream:
    if chunk.choices:
        print(chunk.choices[0].delta.content, end="")
    elif chunk.usage:
        print(f"\nTokens: {chunk.usage.total_tokens}")
```

#### D.2 é˜¿é‡Œäº‘ç™¾ç‚¼æ€è€ƒæ¨¡å¼ç¤ºä¾‹ï¼ˆå®Œæ•´ç‰ˆï¼‰

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# å¼€å¯æ€è€ƒæ¨¡å¼
stream = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "è¯·è§£é‡Šé‡å­è®¡ç®—"}],
    extra_body={
        "enable_thinking": True,
        "thinking_budget": 100  # å¯é€‰ï¼šé™åˆ¶æ€è€ƒé•¿åº¦
    },
    stream=True,
    stream_options={"include_usage": True}
)

reasoning_content = ""  # æ€è€ƒè¿‡ç¨‹
answer_content = ""     # å›å¤å†…å®¹
is_answering = False

for chunk in stream:
    if not chunk.choices:
        # æœ€åä¸€ä¸ªchunkåŒ…å«usageä¿¡æ¯
        print(f"\n[Tokenæ¶ˆè€—] æ€è€ƒ: {chunk.usage.completion_tokens_details.reasoning_tokens}, å›å¤: {chunk.usage.completion_tokens - chunk.usage.completion_tokens_details.reasoning_tokens}")
        continue
    
    delta = chunk.choices[0].delta
    
    # å¤„ç†æ€è€ƒè¿‡ç¨‹
    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
        if not is_answering:
            print("[æ€è€ƒè¿‡ç¨‹]", end="")
        print(delta.reasoning_content, end="", flush=True)
        reasoning_content += delta.reasoning_content
    
    # å¤„ç†å›å¤å†…å®¹
    if delta.content:
        if not is_answering:
            print("\n[æ­£å¼å›å¤]", end="")
            is_answering = True
        print(delta.content, end="", flush=True)
        answer_content += delta.content
```

#### D.3 å‰ç«¯æ€è€ƒæ¨¡å¼å¤„ç†ç¤ºä¾‹

```javascript
// å‰ç«¯JavaScriptå¤„ç†æ€è€ƒæ¨¡å¼æµå¼å“åº”
async function handleThinkingStream(stream) {
    let reasoningContent = '';
    let answerContent = '';
    let isAnswering = false;
    
    const reasoningDiv = document.getElementById('reasoning');
    const answerDiv = document.getElementById('answer');
    
    for await (const chunk of stream) {
        if (!chunk.choices?.length) {
            // æ˜¾ç¤ºTokenç»Ÿè®¡
            console.log('Usage:', chunk.usage);
            continue;
        }
        
        const delta = chunk.choices[0].delta;
        
        // å¤„ç†æ€è€ƒè¿‡ç¨‹
        if (delta.reasoning_content) {
            reasoningContent += delta.reasoning_content;
            reasoningDiv.textContent = reasoningContent;
        }
        
        // å¤„ç†å›å¤å†…å®¹
        if (delta.content) {
            answerContent += delta.content;
            answerDiv.textContent = answerContent;
        }
    }
}
```

#### D.4 åç«¯ç»Ÿä¸€æ€è€ƒæ¨¡å¼å¤„ç†

```python
# backend/services/llm/thinking_handler.py
class ThinkingModeHandler:
    """æ€è€ƒæ¨¡å¼ç»Ÿä¸€å¤„ç†å™¨"""
    
    @staticmethod
    def process_stream_chunk(chunk, provider: str):
        """
        ç»Ÿä¸€å¤„ç†ä¸åŒæœåŠ¡å•†çš„æ€è€ƒæ¨¡å¼å“åº”
        
        Args:
            chunk: åŸå§‹å“åº”å—
            provider: æœåŠ¡å•†æ ‡è¯†
            
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„å“åº”:
            {
                "type": "reasoning" | "content" | "done",
                "delta": "...",
                "usage": {...}  # ä»…doneæ—¶è¿”å›
            }
        """
        if provider == "aliyun":
            # é˜¿é‡Œäº‘æ€è€ƒæ¨¡å¼å¤„ç†
            if not chunk.choices:
                return {
                    "type": "done",
                    "usage": chunk.usage.model_dump() if chunk.usage else None
                }
            
            delta = chunk.choices[0].delta
            
            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                return {"type": "reasoning", "delta": delta.reasoning_content}
            elif delta.content:
                return {"type": "content", "delta": delta.content}
                
        elif provider == "zhipu":
            # æ™ºè°±AIæš‚ä¸æ”¯æŒæ€è€ƒæ¨¡å¼
            if not chunk.choices:
                return {"type": "done", "usage": chunk.usage.model_dump() if chunk.usage else None}
            
            delta = chunk.choices[0].delta
            if delta.content:
                return {"type": "content", "delta": delta.content}
        
        return {"type": "unknown", "delta": ""}
```

#### D.5 æœåŠ¡å•†åŠŸèƒ½æ£€æµ‹

```python
# backend/services/llm/feature_detector.py

PROVIDER_FEATURES = {
    "zhipu": {
        "web_search": True,
        "ocr": True,
        "thinking": False,
        "multimodal": False,
        "stream": True
    },
    "aliyun": {
        "web_search": True,   # é€šè¿‡ enable_search
        "ocr": False,         # éœ€ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹
        "thinking": True,     # æ··åˆæ€è€ƒæ¨¡å¼
        "thinking_only_models": ["qwq-plus", "deepseek-r1", "kimi-k2-thinking"],
        "multimodal": True,
        "stream": True
    }
}

def get_supported_features(provider: str, model: str = None) -> dict:
    """è·å–æŒ‡å®šæœåŠ¡å•†/æ¨¡å‹æ”¯æŒçš„åŠŸèƒ½"""
    features = PROVIDER_FEATURES.get(provider, {})
    
    # ç‰¹æ®Šæ¨¡å‹å¤„ç†
    if provider == "aliyun" and model:
        if model in features.get("thinking_only_models", []):
            features["thinking"] = True
            features["thinking_can_disable"] = False  # ä»…æ€è€ƒæ¨¡å¼æ— æ³•å…³é—­
        else:
            features["thinking_can_disable"] = True  # æ··åˆæ€è€ƒæ¨¡å¼å¯å…³é—­
    
    return features

def check_feature_supported(provider: str, feature: str, model: str = None) -> bool:
    """æ£€æŸ¥åŠŸèƒ½æ˜¯å¦æ”¯æŒ"""
    features = get_supported_features(provider, model)
    return features.get(feature, False)
```

#### D.6 ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹

```python
from openai import OpenAI
from typing import Dict, Any

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

class StructuredOutputHandler:
    """ç»“æ„åŒ–è¾“å‡ºå¤„ç†å™¨"""
    
    @staticmethod
    def extract_patent_info(patent_text: str) -> Dict[str, Any]:
        """ä»ä¸“åˆ©æ–‡æœ¬ä¸­æŠ½å–ç»“æ„åŒ–ä¿¡æ¯"""
        schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "patent_extraction",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string", "description": "ä¸“åˆ©æ ‡é¢˜"},
                        "application_number": {"type": "string", "description": "ç”³è¯·å·"},
                        "applicant": {"type": "string", "description": "ç”³è¯·äºº"},
                        "inventors": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å‘æ˜äººåˆ—è¡¨"
                        },
                        "abstract": {"type": "string", "description": "æ‘˜è¦"},
                        "ipc_class": {"type": "string", "description": "IPCåˆ†ç±»å·"},
                        "claims_count": {"type": "integer", "description": "æƒåˆ©è¦æ±‚æ•°é‡"}
                    },
                    "required": ["title", "applicant", "abstract"]
                }
            }
        }
        
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“åˆ©ä¿¡æ¯æŠ½å–ä¸“å®¶ï¼Œè¯·å‡†ç¡®æå–ä¸“åˆ©ä¿¡æ¯"},
                {"role": "user", "content": f"è¯·ä»ä»¥ä¸‹ä¸“åˆ©æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼š\n\n{patent_text}"}
            ],
            response_format=schema,
            temperature=0.1
        )
        
        import json
        return json.loads(completion.choices[0].message.content)
    
    @staticmethod
    def translate_with_structure(text: str, target_lang: str = "ä¸­æ–‡") -> Dict[str, Any]:
        """ç»“æ„åŒ–ç¿»è¯‘è¾“å‡º"""
        schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "translation_result",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "original_text": {"type": "string"},
                        "translated_text": {"type": "string"},
                        "source_language": {"type": "string"},
                        "target_language": {"type": "string"},
                        "confidence": {"type": "number"},
                        "key_terms": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "original": {"type": "string"},
                                    "translated": {"type": "string"}
                                }
                            }
                        }
                    },
                    "required": ["original_text", "translated_text", "source_language", "target_language"]
                }
            }
        }
        
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "user", "content": f"ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ä¸º{target_lang}ï¼Œå¹¶è¿”å›ç»“æ„åŒ–ç»“æœï¼š\n{text}"}
            ],
            response_format=schema,
            temperature=0.3
        )
        
        import json
        return json.loads(completion.choices[0].message.content)
```

#### D.7 æ‰¹é‡æ¨ç†ç¤ºä¾‹

```python
from openai import OpenAI
import json
import time
from typing import List, Dict, Optional

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, model: str = "qwen-plus"):
        self.model = model
        self.max_requests_per_file = 50000
    
    def create_batch_file(self, tasks: List[Dict], output_path: str) -> str:
        """
        åˆ›å»ºæ‰¹é‡è¯·æ±‚æ–‡ä»¶
        
        Args:
            tasks: [{"id": "xxx", "messages": [...]}, ...]
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if len(tasks) > self.max_requests_per_file:
            raise ValueError(f"è¯·æ±‚æ•°é‡è¶…è¿‡é™åˆ¶: {self.max_requests_per_file}")
        
        with open(output_path, "w", encoding="utf-8") as f:
            for task in tasks:
                request = {
                    "custom_id": task["id"],
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": {
                        "model": self.model,
                        "messages": task["messages"],
                        "temperature": task.get("temperature", 0.7)
                    }
                }
                f.write(json.dumps(request, ensure_ascii=False) + "\n")
        
        return output_path
    
    def submit_batch(self, file_path: str, completion_window: str = "24h") -> str:
        """æäº¤æ‰¹é‡ä»»åŠ¡"""
        # ä¸Šä¼ æ–‡ä»¶
        with open(file_path, "rb") as f:
            batch_file = client.files.create(file=f, purpose="batch")
        
        # åˆ›å»ºæ‰¹é‡ä»»åŠ¡
        batch_job = client.batches.create(
            input_file_id=batch_file.id,
            endpoint="/v1/chat/completions",
            completion_window=completion_window
        )
        
        return batch_job.id
    
    def check_batch_status(self, batch_id: str) -> Dict:
        """æ£€æŸ¥æ‰¹é‡ä»»åŠ¡çŠ¶æ€"""
        batch = client.batches.retrieve(batch_id)
        return {
            "id": batch.id,
            "status": batch.status,
            "request_counts": {
                "total": batch.request_counts.total,
                "completed": batch.request_counts.completed,
                "failed": batch.request_counts.failed
            } if batch.request_counts else None
        }
    
    def get_batch_results(self, batch_id: str, output_path: str) -> List[Dict]:
        """è·å–æ‰¹é‡ä»»åŠ¡ç»“æœ"""
        batch = client.batches.retrieve(batch_id)
        
        if batch.status != "finalized":
            raise ValueError(f"æ‰¹é‡ä»»åŠ¡æœªå®Œæˆï¼Œå½“å‰çŠ¶æ€: {batch.status}")
        
        # ä¸‹è½½ç»“æœæ–‡ä»¶
        result_content = client.files.content(batch.output_file_id)
        results = []
        
        for line in result_content.text.strip().split("\n"):
            if line:
                results.append(json.loads(line))
        
        # ä¿å­˜ç»“æœ
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results

# ä½¿ç”¨ç¤ºä¾‹ï¼šæ‰¹é‡ç¿»è¯‘ä¸“åˆ©æ‘˜è¦
def batch_translate_patents(patents: List[Dict]) -> List[Dict]:
    """æ‰¹é‡ç¿»è¯‘ä¸“åˆ©æ‘˜è¦"""
    processor = BatchProcessor(model="qwen-plus")
    
    # åˆ›å»ºç¿»è¯‘ä»»åŠ¡
    tasks = []
    for patent in patents:
        tasks.append({
            "id": f"translate-{patent['patent_id']}",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸“åˆ©ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å‡†ç¡®ç¿»è¯‘ä¸“åˆ©æ‘˜è¦"},
                {"role": "user", "content": f"ç¿»è¯‘ä»¥ä¸‹ä¸“åˆ©æ‘˜è¦ä¸ºä¸­æ–‡ï¼š\n{patent['abstract']}"}
            ],
            "temperature": 0.3
        })
    
    # åˆ›å»ºæ‰¹é‡æ–‡ä»¶
    batch_file = processor.create_batch_file(tasks, "batch_translate.jsonl")
    
    # æäº¤æ‰¹é‡ä»»åŠ¡
    batch_id = processor.submit_batch(batch_file)
    print(f"æ‰¹é‡ä»»åŠ¡å·²æäº¤ï¼ŒID: {batch_id}")
    
    # è½®è¯¢ç­‰å¾…å®Œæˆ
    while True:
        status = processor.check_batch_status(batch_id)
        print(f"çŠ¶æ€: {status['status']}, å®Œæˆ: {status['request_counts']}")
        
        if status["status"] in ["finalized", "failed", "cancelled"]:
            break
        
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    # è·å–ç»“æœ
    if status["status"] == "finalized":
        results = processor.get_batch_results(batch_id, "batch_results.json")
        return results
    
    return []
```

---

## æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | ç‰ˆæœ¬ | æ›´æ–°å†…å®¹ |
|-----|------|---------|
| 2026-02-28 | v1.0 | åˆå§‹ç‰ˆæœ¬ |
| 2026-02-28 | v1.1 | å‘ç°é˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼Œå¤§å¹…ç®€åŒ–æ•´åˆæ–¹æ¡ˆï¼›æ–°å¢Phase 4.5ç‹¬ç«‹æµ‹è¯•é¡µé¢ï¼›æ›´æ–°APIå·®å¼‚åˆ†æ |
| 2026-02-28 | v1.2 | è¡¥å……æ·±åº¦æ€è€ƒæ¨¡å¼è¯¦ç»†æ–‡æ¡£ï¼šæ··åˆæ€è€ƒvsä»…æ€è€ƒæ¨¡å¼ã€thinking_budgeté™åˆ¶ã€å®Œæ•´ä»£ç ç¤ºä¾‹ï¼›æ–°å¢æœåŠ¡å•†åŠŸèƒ½æ£€æµ‹æ¨¡å—ï¼›æ›´æ–°æµ‹è¯•ç”¨ä¾‹ |
| 2026-02-28 | v1.3 | è¡¥å……ç»“æ„åŒ–è¾“å‡ºæ–‡æ¡£ï¼ˆJSON Object/JSON Schemaæ¨¡å¼ï¼‰ï¼›æ–°å¢æ‰¹é‡æ¨ç†APIæ–‡æ¡£ï¼ˆ50%æŠ˜æ‰£ã€JSONLæ ¼å¼ã€å¼‚æ­¥å¤„ç†ï¼‰ï¼›ä¿®æ­£è”ç½‘æœç´¢æ”¯æŒçŠ¶æ€ï¼ˆé˜¿é‡Œäº‘æ”¯æŒenable_searchï¼‰ï¼›æ·»åŠ D.6/D.7ä»£ç ç¤ºä¾‹ |
