# 阿里云百炼模型盘点

> 文档版本: v1.0  
> 创建日期: 2026-02-28  
> 数据来源: 阿里云百炼官方文档

---

## 一、平台概述

阿里云百炼（Bailian）是阿里云推出的大模型服务平台，提供通义千问系列及多家第三方模型服务。平台支持OpenAI兼容接口，便于开发者快速接入。

**平台特点**:
- 支持OpenAI兼容接口（`https://dashscope.aliyuncs.com/compatible-mode/v1`）
- 提供免费额度（新用户100万Tokens，有效期90天）
- 支持批量推理API（50%折扣）
- 支持深度思考模式
- 支持联网搜索
- 支持结构化输出

---

## 二、通义千问商业模型系列

### 2.1 Qwen Max 系列（旗舰模型）

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen3-max | 262K | 2.5元/M | 10元/M | 最新旗舰，最强能力 |
| qwen3-max-latest | 262K | 2.5元/M | 10元/M | 最新版本，自动更新 |
| qwen-max | 32K | 2.4元/M | 9.6元/M | 稳定版本 |
| qwen-max-longcontext | 1M | 2.4元/M | 9.6元/M | 超长上下文版本 |

**适用场景**: 复杂推理、代码生成、高质量内容创作

### 2.2 Qwen Plus 系列（性价比之选）

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen-plus | 1M | 0.8元/M | 2元/M | 100万上下文，性价比最高 |
| qwen-plus-latest | 1M | 0.8元/M | 2元/M | 最新版本 |

**适用场景**: 日常对话、文档分析、批量处理

### 2.3 Qwen Flash 系列（高速低价）

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen-flash | 1M | 0.15元/M | 1.5元/M | 极速响应，超低成本 |
| qwen-turbo | 128K | 0.3元/M | 0.6元/M | 经典快速模型 |
| qwen-turbo-latest | 128K | 0.3元/M | 0.6元/M | 最新版本 |

**适用场景**: 简单对话、快速响应、大规模调用

---

## 三、Qwen3/Qwen2.5 开源系列

### 3.1 Qwen3 系列

| 模型ID | 参数规模 | 上下文长度 | 输入价格 | 输出价格 |
|--------|---------|-----------|----------|----------|
| qwen3-235b-a22b | 235B(A22B) | 131K | 2.5元/M | 10元/M |
| qwen3-32b | 32B | 131K | 0.5元/M | 2元/M |
| qwen3-14b | 14B | 131K | 0.2元/M | 0.8元/M |
| qwen3-8b | 8B | 131K | 0.05元/M | 0.2元/M |

### 3.2 Qwen2.5 系列

| 模型ID | 参数规模 | 上下文长度 | 输入价格 | 输出价格 |
|--------|---------|-----------|----------|----------|
| qwen2.5-72b-instruct | 72B | 131K | 0.8元/M | 2元/M |
| qwen2.5-32b-instruct | 32B | 131K | 0.35元/M | 0.7元/M |
| qwen2.5-14b-instruct | 14B | 131K | 0.2元/M | 0.6元/M |
| qwen2.5-7b-instruct | 7B | 131K | 0.05元/M | 0.2元/M |

---

## 四、QwQ 推理模型系列

推理模型专为复杂推理任务设计，支持深度思考模式。

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwq-plus | 131K | 1.6元/M | 4元/M | 主力推理模型 |
| qwq-32b | 131K | 0.5元/M | 2元/M | 轻量推理模型 |

**特点**: 
- 仅思考模式（无法关闭思考功能）
- 输出包含 `reasoning_content`（思考过程）和 `content`（最终回复）

---

## 五、多模态模型系列

### 5.1 视觉理解模型

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen-vl-max | 32K | 3元/M | 9元/M | 最强视觉理解能力 |
| qwen-vl-plus | 32K | 1.5元/M | 4.5元/M | 平衡性能与成本 |
| qwen2.5-vl-72b-instruct | 32K | 1.6元/M | 4元/M | 开源视觉模型 |
| qwen2.5-vl-32b-instruct | 32K | 0.7元/M | 2元/M | 轻量视觉模型 |

**支持能力**: 图片理解、OCR、图表分析、视频理解

### 5.2 OCR专用模型

| 模型ID | 特点 |
|--------|------|
| qwen-vl-ocr | 专用OCR模型，支持高精度文字识别 |

---

## 六、代码模型系列

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen-coder-plus | 131K | 0.8元/M | 2元/M | 代码专用，能力强 |
| qwen-coder-turbo | 131K | 0.3元/M | 0.6元/M | 快速代码生成 |
| qwen2.5-coder-32b-instruct | 131K | 0.35元/M | 0.7元/M | 开源代码模型 |

**适用场景**: 代码生成、代码审查、Bug修复、技术文档

---

## 七、翻译模型系列

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| qwen-mt-plus | 32K | 1元/M | 3元/M | 专业翻译模型 |
| qwen-mt-turbo | 32K | 0.3元/M | 0.6元/M | 快速翻译 |

**支持能力**: 多语言翻译、专业术语翻译、专利翻译

---

## 八、第三方模型

阿里云百炼平台还提供多家第三方模型服务：

### 8.1 DeepSeek 系列

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| deepseek-r1 | 16K | 4元/M | 16元/M | 深度推理模型 |
| deepseek-v3 | 64K | 1元/M | 4元/M | 通用大模型 |
| deepseek-v3-0324 | 64K | 1元/M | 4元/M | 最新版本 |
| deepseek-r1-distill-qwen-32b | 131K | 0.5元/M | 2元/M | 蒸馏版推理模型 |
| deepseek-r1-distill-qwen-14b | 131K | 0.2元/M | 0.8元/M | 轻量推理模型 |

### 8.2 Kimi 系列（月之暗面）

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| kimi-k2.5 | 128K | 1元/M | 4元/M | 通用模型 |
| kimi-k2-thinking | 128K | 1.6元/M | 6元/M | 推理模型 |

### 8.3 GLM 系列（智谱AI）

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| glm-5 | 128K | 4元/M | 16元/M | 智谱旗舰模型 |
| glm-4.7 | 128K | 2元/M | 8元/M | 平衡版 |
| glm-4.5 | 128K | 0.8元/M | 3元/M | 性价比版 |

### 8.4 MiniMax 系列

| 模型ID | 上下文长度 | 输入价格 | 输出价格 | 特点 |
|--------|-----------|----------|----------|------|
| minimax-text-01 | 245K | 1元/M | 4元/M | 通用模型 |

---

## 九、定价汇总对比

### 9.1 主力模型价格对比（元/百万Tokens）

| 模型 | 输入价格 | 输出价格 | 上下文 | 推荐场景 |
|------|----------|----------|--------|----------|
| qwen-flash | 0.15 | 1.5 | 1M | 高速低价 |
| qwen-turbo | 0.3 | 0.6 | 128K | 快速响应 |
| qwen-plus | 0.8 | 2 | 1M | 性价比首选 |
| qwen3-max | 2.5 | 10 | 262K | 旗舰能力 |
| qwq-plus | 1.6 | 4 | 131K | 复杂推理 |
| deepseek-r1 | 4 | 16 | 16K | 深度推理 |

### 9.2 免费额度说明

- 新用户注册即送 **100万Tokens** 免费额度
- 有效期 **90天**
- 可用于体验所有模型

### 9.3 批量API折扣

- Batch API 享受 **50%折扣**
- 适用于大规模异步处理场景
- 单文件最多支持 **50,000个请求**

---

## 十、特殊功能说明

### 10.1 深度思考模式

**两种思考模式**:

| 模式 | 说明 | 模型示例 | 控制方式 |
|------|------|---------|----------|
| 混合思考模式 | 可开关思考功能 | qwen-plus, qwen-turbo, qwen-max | `enable_thinking: true/false` |
| 仅思考模式 | 始终开启思考 | qwq-plus, deepseek-r1, kimi-k2-thinking | 无法关闭 |

**使用示例**:
```python
# 开启思考模式
completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "请解释量子计算"}],
    extra_body={"enable_thinking": True},
    stream=True
)

# 响应包含两个字段
# delta.reasoning_content - 思考过程
# delta.content - 最终回复
```

### 10.2 联网搜索

```python
# 开启联网搜索
completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "今天北京天气"}],
    extra_body={"enable_search": True}
)
```

**注意**: 不是所有模型都支持联网搜索，需查看模型文档确认。

### 10.3 结构化输出

支持两种模式：

| 模式 | 说明 | 适用场景 |
|------|------|---------|
| JSON Object模式 | 输出合法JSON，不约束结构 | 灵活数据抽取 |
| JSON Schema模式 | 严格按Schema输出 | 确保字段类型和结构 |

**JSON Schema示例**:
```python
schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "patent_info",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "title": {"type": "string"},
                "applicant": {"type": "string"},
                "inventors": {"type": "array", "items": {"type": "string"}}
            },
            "required": ["title", "applicant"]
        }
    }
}

completion = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "提取专利信息..."}],
    response_format=schema
)
```

### 10.4 批量推理API (Batch API)

**优势**: 50%折扣，适合大规模处理

**流程**:
1. 准备JSONL格式请求文件
2. 上传文件到平台
3. 创建批量任务
4. 等待处理完成
5. 获取结果文件

**JSONL格式示例**:
```jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen-plus", "messages": [{"role": "user", "content": "翻译：Hello"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "qwen-plus", "messages": [{"role": "user", "content": "翻译：World"}]}}
```

---

## 十一、专利工作台模型推荐

### 11.1 按场景推荐

| 使用场景 | 推荐模型 | 原因 |
|---------|---------|------|
| 专利摘要翻译 | qwen-mt-plus / qwen-plus | 专业翻译，性价比高 |
| 权利要求分析 | qwen-plus / qwq-plus | 复杂推理，思考模式 |
| 专利附图识别 | qwen-vl-plus | 视觉理解，OCR能力 |
| 批量专利处理 | qwen-flash + Batch API | 高速低价，50%折扣 |
| 复杂专利解读 | qwen3-max / deepseek-r1 | 最强能力，深度推理 |
| 代码生成（专利管理系统） | qwen-coder-plus | 代码专用 |

### 11.2 成本优化建议

1. **测试阶段**: 使用免费额度体验各模型
2. **日常使用**: qwen-plus 性价比最高
3. **大批量处理**: 使用 Batch API 节省50%
4. **简单任务**: qwen-flash 超低成本
5. **复杂推理**: qwq-plus 比 deepseek-r1 更经济

### 11.3 与智谱AI对比

| 对比项 | 智谱AI | 阿里云百炼 |
|--------|--------|-----------|
| 免费模型 | GLM-4-Flash | 新用户100万Tokens |
| 性价比模型 | GLM-4-FlashX | qwen-flash |
| 旗舰模型 | GLM-5 | qwen3-max |
| 推理模型 | GLM-Z1系列 | qwq-plus / deepseek-r1 |
| 多模态 | GLM-4V | qwen-vl系列 |
| OCR | GLM-OCR | qwen-vl-ocr |
| 联网搜索 | 支持 | 支持 |
| 思考模式 | 不支持 | 支持 |
| Batch API | 支持 | 支持（50%折扣） |

---

## 十二、API调用示例

### 12.1 基础调用

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# 非流式调用
response = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "你好"}]
)
print(response.choices[0].message.content)

# 流式调用
stream = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "你好"}],
    stream=True,
    stream_options={"include_usage": True}
)

for chunk in stream:
    if chunk.choices:
        print(chunk.choices[0].delta.content, end="")
    elif chunk.usage:
        print(f"\nTokens: {chunk.usage.total_tokens}")
```

### 12.2 思考模式调用

```python
stream = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "请分析这个专利的创新点"}],
    extra_body={"enable_thinking": True},
    stream=True
)

for chunk in stream:
    if chunk.choices:
        delta = chunk.choices[0].delta
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            print(f"[思考] {delta.reasoning_content}", end="")
        elif delta.content:
            print(f"[回复] {delta.content}", end="")
```

---

## 参考链接

- 阿里云百炼官网: https://bailian.console.aliyun.com/
- 模型列表文档: https://help.aliyun.com/zh/model-studio/getting-started/models
- 定价文档: https://help.aliyun.com/zh/model-studio/billing-for-model-studio
- OpenAI兼容接口: https://help.aliyun.com/zh/model-studio/compatibility-of-openai-with-dashscope
- API Key获取: https://help.aliyun.com/zh/model-studio/get-api-key
